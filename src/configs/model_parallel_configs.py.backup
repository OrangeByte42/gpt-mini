import os
import torch
from dataclasses import dataclass
from typing import Any, Dict, List


@dataclass
class ModelParallelConfigs:
    """Configuration for Model Parallelism."""
    
    # Model parallel settings
    ENABLE_MODEL_PARALLEL: bool = True
    AUTO_DEVICE_MAP: bool = True  # Automatically create device map
    
    # Device configuration
    FORCE_CPU: bool = False  # Force CPU usage for testing
    PREFERRED_DEVICES: List[int] = None  # List of preferred GPU indices, None for auto
    
    # Memory management
    GRADIENT_CHECKPOINTING: bool = False  # Enable gradient checkpointing to save memory
    MIXED_PRECISION: bool = False  # Enable mixed precision training (FP16)
    
    # Pipeline parallelism settings (for future enhancement)
    PIPELINE_PARALLEL: bool = False
    MICRO_BATCH_SIZE: int = 1  # For pipeline parallelism
    
    # Communication settings
    COMMUNICATION_BACKEND: str = "nccl"  # For multi-node setups
    
    # Debugging and monitoring
    PROFILE_MEMORY: bool = True  # Profile memory usage
    LOG_DEVICE_ASSIGNMENTS: bool = True  # Log device assignments
    
    def __post_init__(self: Any) -> None:
        """Post-initialization setup."""
        # Validate settings
        if self.ENABLE_MODEL_PARALLEL and not torch.cuda.is_available() and not self.FORCE_CPU:
            print("Warning: Model parallelism enabled but CUDA not available. Falling back to CPU.")
            self.FORCE_CPU = True
        
        # Set default preferred devices if not specified
        if self.PREFERRED_DEVICES is None and torch.cuda.is_available():
            self.PREFERRED_DEVICES = list(range(torch.cuda.device_count()))
    
    def get_available_devices(self: Any) -> List[torch.device]:
        """Get list of available devices based on configuration."""
        if self.FORCE_CPU or not torch.cuda.is_available():
            return [torch.device('cpu')]
        
        devices = []
        if self.PREFERRED_DEVICES:
            for device_idx in self.PREFERRED_DEVICES:
                if device_idx < torch.cuda.device_count():
                    devices.append(torch.device(f'cuda:{device_idx}'))
        
        # Fallback to all available devices if no valid preferred devices
        if not devices:
            devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]
        
        return devices
    
    def get_device_info(self: Any) -> Dict[str, Any]:
        """Get information about available devices."""
        info = {
            'cuda_available': torch.cuda.is_available(),
            'device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'available_devices': self.get_available_devices(),
            'force_cpu': self.FORCE_CPU,
            'preferred_devices': self.PREFERRED_DEVICES
        }
        
        if torch.cuda.is_available():
            info['device_names'] = [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]
            info['device_memory'] = []
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                info['device_memory'].append({
                    'device': i,
                    'total_memory_gb': props.total_memory / 1024**3,
                    'name': props.name
                })
        
        return info


def create_balanced_device_map(num_layers: int, 
                              available_devices: List[torch.device], 
                              embedding_device: torch.device = None,
                              output_device: torch.device = None) -> Dict[str, torch.device]:
    """Create a balanced device map for model parallelism.
    
    @param num_layers: Number of transformer layers
    @param available_devices: List of available devices
    @param embedding_device: Specific device for embedding layer (optional)
    @param output_device: Specific device for output layer (optional)
    @return: Dictionary mapping component names to devices
    """
    if len(available_devices) == 0:
        raise ValueError("No available devices provided")
    
    device_map = {}
    
    # Set embedding device
    if embedding_device is None:
        embedding_device = available_devices[0]
    device_map['embedding'] = embedding_device
    
    # Set output device
    if output_device is None:
        output_device = available_devices[-1]
    device_map['output'] = output_device
    
    # Distribute layers across devices
    if len(available_devices) == 1:
        # Single device - put everything on it
        for i in range(num_layers):
            device_map[f'layer_{i}'] = available_devices[0]
    else:
        # Multi-device - distribute layers evenly
        layers_per_device = num_layers // len(available_devices)
        remainder = num_layers % len(available_devices)
        
        layer_idx = 0
        for device_idx, device in enumerate(available_devices):
            # Calculate how many layers for this device
            num_layers_this_device = layers_per_device
            if device_idx < remainder:
                num_layers_this_device += 1
            
            # Assign layers to this device
            for _ in range(num_layers_this_device):
                if layer_idx < num_layers:
                    device_map[f'layer_{layer_idx}'] = device
                    layer_idx += 1
    
    return device_map


def create_memory_optimized_device_map(num_layers: int,
                                     available_devices: List[torch.device],
                                     layer_memory_weights: List[float] = None) -> Dict[str, torch.device]:
    """Create a memory-optimized device map based on layer memory requirements.
    
    @param num_layers: Number of transformer layers
    @param available_devices: List of available devices
    @param layer_memory_weights: Memory weights for each layer (optional)
    @return: Dictionary mapping component names to devices
    """
    if not torch.cuda.is_available():
        # CPU fallback
        return create_balanced_device_map(num_layers, available_devices)
    
    # Get memory information for each device
    device_memory = []
    for device in available_devices:
        if device.type == 'cuda':
            props = torch.cuda.get_device_properties(device)
            device_memory.append(props.total_memory)
        else:
            device_memory.append(float('inf'))  # CPU has "unlimited" memory
    
    # If no layer weights provided, assume uniform distribution
    if layer_memory_weights is None:
        layer_memory_weights = [1.0] * num_layers
    
    # Create device assignments based on memory capacity
    device_map = {}
    device_loads = [0.0] * len(available_devices)
    
    # Embedding and output layers (assume they use standard memory)
    embedding_device_idx = 0  # Put on first device
    output_device_idx = len(available_devices) - 1  # Put on last device
    
    device_map['embedding'] = available_devices[embedding_device_idx]
    device_map['output'] = available_devices[output_device_idx]
    
    # Add base load for embedding and output
    device_loads[embedding_device_idx] += 1.0
    device_loads[output_device_idx] += 1.0
    
    # Assign layers to devices based on current load
    for layer_idx in range(num_layers):
        # Find device with minimum load (relative to its capacity)
        relative_loads = [load / memory for load, memory in zip(device_loads, device_memory)]
        min_device_idx = relative_loads.index(min(relative_loads))
        
        device_map[f'layer_{layer_idx}'] = available_devices[min_device_idx]
        device_loads[min_device_idx] += layer_memory_weights[layer_idx]
    
    return device_map


def print_device_map(device_map: Dict[str, torch.device], configs: ModelParallelConfigs = None) -> None:
    """Print device map information in a formatted way."""
    print("=" * 60)
    print("MODEL PARALLEL DEVICE ASSIGNMENT")
    print("=" * 60)
    
    # Group by device
    device_groups = {}
    for component, device in device_map.items():
        device_str = str(device)
        if device_str not in device_groups:
            device_groups[device_str] = []
        device_groups[device_str].append(component)
    
    # Print grouped information
    for device_str, components in device_groups.items():
        print(f"\n{device_str}:")
        for component in sorted(components):
            print(f"  - {component}")
        
        # Add memory info if available
        if configs and torch.cuda.is_available() and 'cuda' in device_str:
            device_idx = int(device_str.split(':')[1])
            props = torch.cuda.get_device_properties(device_idx)
            print(f"  Memory: {props.total_memory / 1024**3:.1f} GB ({props.name})")
    
    print("=" * 60)


# Example usage functions
def get_recommended_batch_size(device_map: Dict[str, torch.device], 
                             model_size_gb: float = 1.0) -> int:
    """Get recommended batch size based on device memory and model size."""
    if not torch.cuda.is_available():
        return 4  # Conservative batch size for CPU
    
    # Find the device with minimum available memory
    min_memory_gb = float('inf')
    for device in device_map.values():
        if device.type == 'cuda':
            props = torch.cuda.get_device_properties(device)
            memory_gb = props.total_memory / 1024**3
            min_memory_gb = min(min_memory_gb, memory_gb)
    
    if min_memory_gb == float('inf'):
        return 4
    
    # Reserve memory for model parameters, optimizer states, and gradients
    # Rule of thumb: model + optimizer states â‰ˆ 3x model size
    # Add buffer for activations and intermediate results
    available_memory = min_memory_gb - (model_size_gb * 3) - 2  # 2GB buffer
    
    if available_memory <= 0:
        return 1  # Minimum batch size
    
    # Estimate memory per batch item (rough approximation)
    memory_per_batch = 0.5  # GB per batch item (adjust based on sequence length)
    
    recommended_batch_size = max(1, int(available_memory / memory_per_batch))
    
    # Cap at reasonable maximum
    return min(recommended_batch_size, 32)

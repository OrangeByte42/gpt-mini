{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, List\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be75440",
   "metadata": {},
   "source": [
    "# Download & Pre-process Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a8f51",
   "metadata": {},
   "source": [
    "## open-text-books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73137483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set local name\n",
    "local_name: str = \"open-text-books\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download original dataset\n",
    "ori_save_dir: str = os.path.join(ori_datasets_dir, local_name)\n",
    "os.makedirs(ori_save_dir, exist_ok=True)\n",
    "\n",
    "open_text_books: Any = download_ori_dataset(\"izumi-lab/open-text-books\", cache_dir=ori_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce6a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic information\n",
    "print(type(open_text_books), open_text_books.keys())\n",
    "print(type(open_text_books[\"train\"]), type(open_text_books[\"train\"][0]))\n",
    "print(open_text_books[\"train\"][0].keys(), type(open_text_books[\"train\"][0][\"text\"]))\n",
    "print(open_text_books[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only text for training\n",
    "open_text_books = open_text_books[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13997893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show length distribution\n",
    "show_sample_length_distribution(open_text_books, split_percent=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process: split samples by paragraphs\n",
    "open_text_books = split_sample_by_paragraphs(open_text_books, threshold_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show length distribution again\n",
    "show_sample_length_distribution(open_text_books, split_percent=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e3813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some samples after pre-processing\n",
    "print(\"\\n\\n\".join(open_text_books[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pre-processed data as Parquet files\n",
    "batch_size: int = 10_000_000\n",
    "save_dir: str = os.path.join(preprocessed_datasets_dir, local_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_as_parquet(local_name, open_text_books, batch_size=batch_size, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3a168",
   "metadata": {},
   "source": [
    "## c4-subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef02c3",
   "metadata": {},
   "source": [
    "## GPUåŠ é€Ÿé¢„å¤„ç†å·¥å…·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d75db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUåŠ é€Ÿé¢„å¤„ç†ç¯å¢ƒæ£€æµ‹\n",
    "import torch\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# æ£€æµ‹æ˜¯å¦æœ‰GPUå¯ç”¨\n",
    "gpu_available = torch.cuda.is_available()\n",
    "device_count = torch.cuda.device_count() if gpu_available else 0\n",
    "cpu_count = mp.cpu_count()\n",
    "\n",
    "print(f\"GPUå¯ç”¨: {gpu_available}\")\n",
    "if gpu_available:\n",
    "    print(f\"GPUæ•°é‡: {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"GPU {i} å†…å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"CPUæ ¸å¿ƒæ•°: {cpu_count}\")\n",
    "\n",
    "# è®¾ç½®å¤„ç†ç­–ç•¥\n",
    "use_gpu = gpu_available and device_count > 0\n",
    "use_multiprocess = cpu_count > 1\n",
    "print(f\"ä½¿ç”¨GPUåŠ é€Ÿ: {use_gpu}\")\n",
    "print(f\"ä½¿ç”¨å¤šè¿›ç¨‹: {use_multiprocess}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f965795e",
   "metadata": {},
   "source": [
    "### å¯é€‰ï¼šå®‰è£…GPUåŠ é€Ÿåº“\n",
    "\n",
    "å¦‚æœè¦ä½¿ç”¨GPUåŠ é€Ÿï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…æ‰€éœ€çš„åº“ï¼š\n",
    "\n",
    "```bash\n",
    "# å®‰è£…RAPIDSåº“ï¼ˆåŒ…å«cuDFï¼‰\n",
    "conda install -c rapidsai -c conda-forge -c nvidia cudf python=3.9 cudatoolkit=11.8\n",
    "\n",
    "# æˆ–è€…ä½¿ç”¨pipå®‰è£…ï¼ˆéœ€è¦å·²å®‰è£…CUDAï¼‰\n",
    "pip install cudf-cu11 --extra-index-url=https://pypi.nvidia.com\n",
    "pip install cupy-cuda11x\n",
    "```\n",
    "\n",
    "å¦‚æœæ²¡æœ‰GPUæˆ–å®‰è£…å¤±è´¥ï¼Œä»£ç ä¼šè‡ªåŠ¨å›é€€åˆ°CPUå¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_streaming(dataset_name: str, cache_dir: str, streaming_chunk_size: int = 1000):\n",
    "    \"\"\"æµå¼åŠ è½½æ•°æ®é›†ï¼Œé¿å…å†…å­˜æº¢å‡º\"\"\"\n",
    "    print(f\"å¼€å§‹æµå¼åŠ è½½æ•°æ®é›†: {dataset_name}\")\n",
    "    \n",
    "    # ä½¿ç”¨streamingæ¨¡å¼åŠ è½½æ•°æ®é›†\n",
    "    dataset = datasets.load_dataset(dataset_name, cache_dir=cache_dir, streaming=True)\n",
    "    \n",
    "    # å°†æµå¼æ•°æ®é›†è½¬æ¢ä¸ºè¿­ä»£å™¨\n",
    "    train_iter = iter(dataset[\"train\"])\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            for _ in range(streaming_chunk_size):\n",
    "                try:\n",
    "                    item = next(train_iter)\n",
    "                    current_chunk.append(item[\"text\"])\n",
    "                except StopIteration:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(current_chunk)\n",
    "                    raise StopIteration\n",
    "            \n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            print(f\"å·²åŠ è½½ {len(chunks)} ä¸ªæ•°æ®å—ï¼Œæ¯å— {streaming_chunk_size} ä¸ªæ ·æœ¬\")\n",
    "            \n",
    "    except StopIteration:\n",
    "        print(f\"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œæ€»å…± {len(chunks)} ä¸ªæ•°æ®å—\")\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ed013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_chunk_gpu(texts: List[str], threshold_length: int = 512) -> List[str]:\n",
    "    \"\"\"ä½¿ç”¨GPUåŠ é€Ÿå¤„ç†æ–‡æœ¬å—\"\"\"\n",
    "    if not use_gpu:\n",
    "        return process_text_chunk_cpu(texts, threshold_length)\n",
    "    \n",
    "    try:\n",
    "        # å°è¯•ä½¿ç”¨cuDFè¿›è¡ŒGPUåŠ é€Ÿ\n",
    "        import cudf\n",
    "        import cupy as cp\n",
    "        \n",
    "        # è½¬æ¢ä¸ºGPU DataFrame\n",
    "        df = cudf.DataFrame({\"text\": texts})\n",
    "        \n",
    "        # è®¡ç®—æ–‡æœ¬é•¿åº¦\n",
    "        df[\"length\"] = df[\"text\"].str.len()\n",
    "        \n",
    "        # åˆ†ç¦»çŸ­æ–‡æœ¬å’Œé•¿æ–‡æœ¬\n",
    "        short_texts = df[df[\"length\"] <= threshold_length][\"text\"].to_pandas().tolist()\n",
    "        long_texts = df[df[\"length\"] > threshold_length][\"text\"].to_pandas().tolist()\n",
    "        \n",
    "        # å¯¹é•¿æ–‡æœ¬è¿›è¡Œåˆ†å‰²\n",
    "        paragraphs = short_texts.copy()\n",
    "        for text in long_texts:\n",
    "            # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²\n",
    "            split_texts = [t.strip() for t in text.split(\"\\\\n\") if t.strip()]\n",
    "            paragraphs.extend(split_texts)\n",
    "        \n",
    "        return paragraphs\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"cuDFä¸å¯ç”¨ï¼Œå›é€€åˆ°CPUå¤„ç†\")\n",
    "        return process_text_chunk_cpu(texts, threshold_length)\n",
    "    except Exception as e:\n",
    "        print(f\"GPUå¤„ç†å‡ºé”™: {e}ï¼Œå›é€€åˆ°CPUå¤„ç†\")\n",
    "        return process_text_chunk_cpu(texts, threshold_length)\n",
    "\n",
    "def process_text_chunk_cpu(texts: List[str], threshold_length: int = 512) -> List[str]:\n",
    "    \"\"\"CPUå¤„ç†æ–‡æœ¬å—\"\"\"\n",
    "    paragraphs = []\n",
    "    \n",
    "    for text in texts:\n",
    "        if len(text) <= threshold_length:\n",
    "            paragraphs.append(text)\n",
    "        else:\n",
    "            # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²\n",
    "            split_texts = [t.strip() for t in text.split(\"\\\\n\") if t.strip()]\n",
    "            paragraphs.extend(split_texts)\n",
    "    \n",
    "    # è¿‡æ»¤ç©ºæ®µè½\n",
    "    return [p for p in paragraphs if len(p) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunks_parallel(chunks: List[List[str]], threshold_length: int = 512, max_workers: int = None) -> List[str]:\n",
    "    \"\"\"å¹¶è¡Œå¤„ç†æ•°æ®å—\"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = min(cpu_count, len(chunks))\n",
    "    \n",
    "    print(f\"ä½¿ç”¨ {max_workers} ä¸ªè¿›ç¨‹å¹¶è¡Œå¤„ç† {len(chunks)} ä¸ªæ•°æ®å—\")\n",
    "    \n",
    "    all_paragraphs = []\n",
    "    \n",
    "    if use_multiprocess and len(chunks) > 1:\n",
    "        # å¤šè¿›ç¨‹å¤„ç†\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # æäº¤æ‰€æœ‰ä»»åŠ¡\n",
    "            future_to_chunk = {\n",
    "                executor.submit(process_text_chunk_gpu, chunk, threshold_length): i \n",
    "                for i, chunk in enumerate(chunks)\n",
    "            }\n",
    "            \n",
    "            # æ”¶é›†ç»“æœ\n",
    "            with tqdm(total=len(chunks), desc=\"å¤„ç†æ•°æ®å—\") as pbar:\n",
    "                for future in as_completed(future_to_chunk):\n",
    "                    chunk_idx = future_to_chunk[future]\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        all_paragraphs.extend(result)\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_postfix({\"å·²å¤„ç†æ®µè½\": len(all_paragraphs)})\n",
    "                    except Exception as e:\n",
    "                        print(f\"å¤„ç†æ•°æ®å— {chunk_idx} æ—¶å‡ºé”™: {e}\")\n",
    "    else:\n",
    "        # å•è¿›ç¨‹å¤„ç†\n",
    "        for i, chunk in enumerate(tqdm(chunks, desc=\"å¤„ç†æ•°æ®å—\")):\n",
    "            try:\n",
    "                result = process_text_chunk_gpu(chunk, threshold_length)\n",
    "                all_paragraphs.extend(result)\n",
    "            except Exception as e:\n",
    "                print(f\"å¤„ç†æ•°æ®å— {i} æ—¶å‡ºé”™: {e}\")\n",
    "    \n",
    "    print(f\"å¤„ç†å®Œæˆï¼Œæ€»å…±å¾—åˆ° {len(all_paragraphs):,} ä¸ªæ®µè½\")\n",
    "    return all_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be72285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_paragraphs_incrementally(paragraphs: List[str], local_name: str, save_dir: str, \n",
    "                                  batch_size: int = 50000, start_part: int = 0) -> None:\n",
    "    \"\"\"æ¸è¿›å¼ä¿å­˜æ®µè½ï¼Œé¿å…å†…å­˜ç§¯ç´¯\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    total_batches = (len(paragraphs) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(paragraphs), batch_size):\n",
    "        batch = paragraphs[i:i + batch_size]\n",
    "        part_num = start_part + i // batch_size\n",
    "        \n",
    "        # åˆ›å»ºæ•°æ®é›†\n",
    "        batch_dataset = Dataset.from_dict({\"text\": batch})\n",
    "        \n",
    "        # ä¿å­˜è·¯å¾„\n",
    "        batch_save_path = os.path.join(save_dir, f\"{local_name}_part_{part_num}.parquet\")\n",
    "        \n",
    "        # ä¿å­˜\n",
    "        batch_dataset.to_parquet(batch_save_path)\n",
    "        \n",
    "        print(f\"å·²ä¿å­˜ç¬¬ {i // batch_size + 1}/{total_batches} æ‰¹æ¬¡åˆ° {batch_save_path}ï¼ŒåŒ…å« {len(batch):,} ä¸ªæ ·æœ¬\")\n",
    "        \n",
    "        # å¼ºåˆ¶åƒåœ¾å›æ”¶\n",
    "        del batch, batch_dataset\n",
    "        import gc\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91c3d70",
   "metadata": {},
   "source": [
    "## c4-subset (GPUåŠ é€Ÿç‰ˆæœ¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304352b",
   "metadata": {},
   "source": [
    "## c4-subset (å¤šGPUè¶…å¿«ç‰ˆæœ¬) ğŸš€ğŸš€ğŸš€ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0849ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šGPUè¶…å¿«å¤„ç†å·¥å…·\n",
    "import threading\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def process_text_chunk_multi_gpu(texts: List[str], threshold_length: int = 512, device_id: int = 0) -> List[str]:\n",
    "    \"\"\"åœ¨æŒ‡å®šGPUä¸Šå¤„ç†æ–‡æœ¬å—\"\"\"\n",
    "    try:\n",
    "        import cudf\n",
    "        import cupy as cp\n",
    "        \n",
    "        # è®¾ç½®å½“å‰GPUè®¾å¤‡\n",
    "        cp.cuda.Device(device_id).use()\n",
    "        \n",
    "        # è½¬æ¢ä¸ºGPU DataFrame\n",
    "        df = cudf.DataFrame({\"text\": texts})\n",
    "        \n",
    "        # è®¡ç®—æ–‡æœ¬é•¿åº¦\n",
    "        df[\"length\"] = df[\"text\"].str.len()\n",
    "        \n",
    "        # åˆ†ç¦»çŸ­æ–‡æœ¬å’Œé•¿æ–‡æœ¬\n",
    "        short_texts = df[df[\"length\"] <= threshold_length][\"text\"].to_pandas().tolist()\n",
    "        long_texts = df[df[\"length\"] > threshold_length][\"text\"].to_pandas().tolist()\n",
    "        \n",
    "        # å¯¹é•¿æ–‡æœ¬è¿›è¡Œåˆ†å‰²\n",
    "        paragraphs = short_texts.copy()\n",
    "        for text in long_texts:\n",
    "            # æŒ‰æ¢è¡Œç¬¦åˆ†å‰²\n",
    "            split_texts = [t.strip() for t in text.split(\"\\n\") if t.strip()]\n",
    "            paragraphs.extend(split_texts)\n",
    "        \n",
    "        return paragraphs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"GPU {device_id} å¤„ç†å‡ºé”™: {e}ï¼Œå›é€€åˆ°CPUå¤„ç†\")\n",
    "        return process_text_chunk_cpu(texts, threshold_length)\n",
    "\n",
    "def multi_gpu_worker(gpu_id: int, input_queue: queue.Queue, output_queue: queue.Queue, threshold_length: int):\n",
    "    \"\"\"å¤šGPUå·¥ä½œè¿›ç¨‹\"\"\"\n",
    "    print(f\"GPU {gpu_id} å·¥ä½œè¿›ç¨‹å¯åŠ¨\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # è·å–ä»»åŠ¡\n",
    "            task = input_queue.get(timeout=5)\n",
    "            if task is None:  # ç»“æŸä¿¡å·\n",
    "                break\n",
    "                \n",
    "            chunk_id, texts = task\n",
    "            \n",
    "            # å¤„ç†æ–‡æœ¬\n",
    "            start_time = time.time()\n",
    "            result = process_text_chunk_multi_gpu(texts, threshold_length, gpu_id)\n",
    "            process_time = time.time() - start_time\n",
    "            \n",
    "            # è¿”å›ç»“æœ\n",
    "            output_queue.put((chunk_id, result, gpu_id, process_time))\n",
    "            input_queue.task_done()\n",
    "            \n",
    "        except queue.Empty:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"GPU {gpu_id} å·¥ä½œè¿›ç¨‹å‡ºé”™: {e}\")\n",
    "            input_queue.task_done()\n",
    "    \n",
    "    print(f\"GPU {gpu_id} å·¥ä½œè¿›ç¨‹ç»“æŸ\")\n",
    "\n",
    "def process_with_multi_gpu(chunks: List[List[str]], threshold_length: int = 512, num_gpus: int = 4) -> List[str]:\n",
    "    \"\"\"ä½¿ç”¨å¤šGPUå¹¶è¡Œå¤„ç†æ•°æ®å—\"\"\"\n",
    "    if not use_gpu or device_count < num_gpus:\n",
    "        print(f\"GPUä¸è¶³ï¼Œéœ€è¦{num_gpus}å¼ å¡ï¼Œå®é™…{device_count}å¼ ï¼Œå›é€€åˆ°å•GPU/CPUå¤„ç†\")\n",
    "        return process_chunks_parallel(chunks, threshold_length)\n",
    "    \n",
    "    print(f\"ğŸš€ å¯åŠ¨ {num_gpus} å¼ GPUè¶…å¿«å¤„ç†æ¨¡å¼!\")\n",
    "    \n",
    "    # åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—å’Œç»“æœé˜Ÿåˆ—\n",
    "    input_queue = queue.Queue(maxsize=num_gpus * 2)  # é™åˆ¶é˜Ÿåˆ—å¤§å°é¿å…å†…å­˜æº¢å‡º\n",
    "    output_queue = queue.Queue()\n",
    "    \n",
    "    # å¯åŠ¨GPUå·¥ä½œçº¿ç¨‹\n",
    "    workers = []\n",
    "    for gpu_id in range(num_gpus):\n",
    "        worker = threading.Thread(\n",
    "            target=multi_gpu_worker, \n",
    "            args=(gpu_id, input_queue, output_queue, threshold_length)\n",
    "        )\n",
    "        worker.start()\n",
    "        workers.append(worker)\n",
    "    \n",
    "    # åˆ›å»ºä»»åŠ¡æäº¤çº¿ç¨‹\n",
    "    def submit_tasks():\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            input_queue.put((i, chunk))\n",
    "        \n",
    "        # å‘é€ç»“æŸä¿¡å·\n",
    "        for _ in range(num_gpus):\n",
    "            input_queue.put(None)\n",
    "    \n",
    "    submit_thread = threading.Thread(target=submit_tasks)\n",
    "    submit_thread.start()\n",
    "    \n",
    "    # æ”¶é›†ç»“æœ\n",
    "    all_paragraphs = []\n",
    "    results = {}\n",
    "    gpu_stats = {i: {\"chunks\": 0, \"time\": 0} for i in range(num_gpus)}\n",
    "    \n",
    "    with tqdm(total=len(chunks), desc=\"å¤šGPUå¤„ç†ä¸­\") as pbar:\n",
    "        for _ in range(len(chunks)):\n",
    "            chunk_id, result, gpu_id, process_time = output_queue.get()\n",
    "            results[chunk_id] = result\n",
    "            gpu_stats[gpu_id][\"chunks\"] += 1\n",
    "            gpu_stats[gpu_id][\"time\"] += process_time\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # æŒ‰é¡ºåºåˆå¹¶ç»“æœ\n",
    "    for i in range(len(chunks)):\n",
    "        all_paragraphs.extend(results[i])\n",
    "    \n",
    "    # ç­‰å¾…æ‰€æœ‰å·¥ä½œçº¿ç¨‹ç»“æŸ\n",
    "    submit_thread.join()\n",
    "    for worker in workers:\n",
    "        worker.join()\n",
    "    \n",
    "    # æ˜¾ç¤ºGPUç»Ÿè®¡ä¿¡æ¯\n",
    "    print(f\"\\nğŸ¯ å¤šGPUå¤„ç†å®Œæˆç»Ÿè®¡:\")\n",
    "    total_time = 0\n",
    "    for gpu_id, stats in gpu_stats.items():\n",
    "        avg_time = stats[\"time\"] / max(stats[\"chunks\"], 1)\n",
    "        total_time += stats[\"time\"]\n",
    "        print(f\"  GPU {gpu_id}: å¤„ç†äº† {stats['chunks']} ä¸ªå—, å¹³å‡è€—æ—¶ {avg_time:.2f}s/å—\")\n",
    "    \n",
    "    print(f\"  æ€»å¤„ç†æ—¶é—´: {total_time:.2f}s\")\n",
    "    print(f\"  å¹³å‡æ¯GPU: {total_time/num_gpus:.2f}s\")\n",
    "    print(f\"  æ€»å…±å¾—åˆ° {len(all_paragraphs):,} ä¸ªæ®µè½\")\n",
    "    \n",
    "    return all_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50741bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ğŸš€ğŸš€ğŸš€ å¤šGPUè¶…å¿«ç‰ˆæœ¬å¤„ç†\n",
    "local_name_ultra: str = \"c4_15m_ultra\"\n",
    "\n",
    "# ä¼˜åŒ–å‚æ•°è®¾ç½®ï¼ˆ4GPUç‰ˆæœ¬ï¼‰\n",
    "streaming_chunk_size = 500_000  # å¢å¤§chunkä»¥å……åˆ†åˆ©ç”¨GPU\n",
    "processing_threshold = 512\n",
    "save_batch_size = 2_000_000    # å¢å¤§ä¿å­˜æ‰¹æ¬¡\n",
    "num_gpus = 4                 # ä½¿ç”¨4å¼ GPU\n",
    "\n",
    "print(f\"ğŸš€ğŸš€ğŸš€ğŸš€ å¯åŠ¨4GPUè¶…å¿«å¤„ç†æ¨¡å¼!\")\n",
    "print(f\"æµå¼å—å¤§å°: {streaming_chunk_size:,}\")\n",
    "print(f\"æ–‡æœ¬é•¿åº¦é˜ˆå€¼: {processing_threshold}\")\n",
    "print(f\"ä¿å­˜æ‰¹æ¬¡å¤§å°: {save_batch_size:,}\")\n",
    "print(f\"ä½¿ç”¨GPUæ•°é‡: {num_gpus}\")\n",
    "\n",
    "# è®¾ç½®ä¿å­˜ç›®å½•\n",
    "ori_save_dir_ultra = os.path.join(ori_datasets_dir, local_name_ultra)\n",
    "preprocessed_save_dir_ultra = os.path.join(preprocessed_datasets_dir, local_name_ultra)\n",
    "os.makedirs(ori_save_dir_ultra, exist_ok=True)\n",
    "os.makedirs(preprocessed_save_dir_ultra, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236ab18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ğŸš€ğŸš€ğŸš€ å¤šGPUæµå¼å¤„ç†ä¸»å¾ªç¯\n",
    "try:\n",
    "    print(\"ğŸš€ å¼€å§‹å¤šGPUè¶…å¿«æµå¼å¤„ç†...\")\n",
    "    \n",
    "    # ä½¿ç”¨streamingæ¨¡å¼åŠ è½½æ•°æ®é›†\n",
    "    dataset_stream = datasets.load_dataset(\"teven/c4_15M\", cache_dir=ori_save_dir_ultra, streaming=True)\n",
    "    train_stream = dataset_stream[\"train\"]\n",
    "    \n",
    "    # åˆå§‹åŒ–è®¡æ•°å™¨å’Œç¼“å­˜\n",
    "    total_processed = 0\n",
    "    total_saved_parts = 0\n",
    "    chunk_buffer = []\n",
    "    processed_buffer = []\n",
    "    \n",
    "    # æ€§èƒ½ç»Ÿè®¡\n",
    "    start_time = time.time()\n",
    "    last_report_time = start_time\n",
    "    \n",
    "    print(\"ğŸ”¥ å¼€å§‹æµå¼å¤„ç†...\")\n",
    "    \n",
    "    # æµå¼å¤„ç†æ•°æ®\n",
    "    for i, item in enumerate(tqdm(train_stream, desc=\"ğŸš€å¤šGPUè¶…å¿«å¤„ç†\")):\n",
    "        chunk_buffer.append(item[\"text\"])\n",
    "        \n",
    "        # å½“ç¼“å­˜è¾¾åˆ°chunkå¤§å°æ—¶ï¼Œè¿›è¡Œå¤„ç†\n",
    "        if len(chunk_buffer) >= streaming_chunk_size:\n",
    "            # å°†æ•°æ®åˆ†æˆ4ä¸ªå­chunkï¼Œæ¯ä¸ªGPUå¤„ç†ä¸€ä¸ª\n",
    "            sub_chunk_size = len(chunk_buffer) // num_gpus\n",
    "            chunks = [\n",
    "                chunk_buffer[j:j+sub_chunk_size] \n",
    "                for j in range(0, len(chunk_buffer), sub_chunk_size)\n",
    "            ]\n",
    "            \n",
    "            # å¦‚æœæœ‰å‰©ä½™æ•°æ®ï¼Œæ·»åŠ åˆ°æœ€åä¸€ä¸ªchunk\n",
    "            if len(chunks) > num_gpus:\n",
    "                chunks[num_gpus-1].extend(chunks[num_gpus])\n",
    "                chunks = chunks[:num_gpus]\n",
    "            \n",
    "            # å¤šGPUå¹¶è¡Œå¤„ç†\n",
    "            chunk_start_time = time.time()\n",
    "            processed_paragraphs = process_with_multi_gpu(chunks, processing_threshold, num_gpus)\n",
    "            chunk_process_time = time.time() - chunk_start_time\n",
    "            \n",
    "            # æ·»åŠ åˆ°ç¼“å­˜\n",
    "            processed_buffer.extend(processed_paragraphs)\n",
    "            \n",
    "            # å¦‚æœå¤„ç†åçš„æ®µè½è¶³å¤Ÿå¤šï¼Œå°±ä¿å­˜\n",
    "            if len(processed_buffer) >= save_batch_size:\n",
    "                save_paragraphs_incrementally(\n",
    "                    processed_buffer, \n",
    "                    local_name_ultra, \n",
    "                    preprocessed_save_dir_ultra,\n",
    "                    batch_size=save_batch_size,\n",
    "                    start_part=total_saved_parts\n",
    "                )\n",
    "                total_saved_parts += (len(processed_buffer) + save_batch_size - 1) // save_batch_size\n",
    "                processed_buffer.clear()\n",
    "            \n",
    "            total_processed += len(chunk_buffer)\n",
    "            chunk_buffer.clear()\n",
    "            \n",
    "            # å†…å­˜æ¸…ç†\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            \n",
    "            # æ€§èƒ½æŠ¥å‘Š\n",
    "            current_time = time.time()\n",
    "            if current_time - last_report_time >= 30:  # æ¯30ç§’æŠ¥å‘Šä¸€æ¬¡\n",
    "                elapsed_time = current_time - start_time\n",
    "                avg_speed = total_processed / elapsed_time\n",
    "                chunk_speed = streaming_chunk_size / chunk_process_time\n",
    "                \n",
    "                print(f\"\\nğŸ“Š æ€§èƒ½æŠ¥å‘Š:\")\n",
    "                print(f\"  å·²å¤„ç†: {total_processed:,} ä¸ªåŸå§‹æ ·æœ¬\")\n",
    "                print(f\"  å·²ä¿å­˜: {total_saved_parts} ä¸ªparquetæ–‡ä»¶\")\n",
    "                print(f\"  å¹³å‡é€Ÿåº¦: {avg_speed:.0f} æ ·æœ¬/ç§’\")\n",
    "                print(f\"  å½“å‰å—é€Ÿåº¦: {chunk_speed:.0f} æ ·æœ¬/ç§’\")\n",
    "                print(f\"  é¢„è®¡4GPUåŠ é€Ÿæ¯”: {chunk_speed/avg_speed:.1f}x\")\n",
    "                \n",
    "                last_report_time = current_time\n",
    "    \n",
    "    # å¤„ç†å‰©ä½™çš„æ•°æ®\n",
    "    if chunk_buffer:\n",
    "        # å¤„ç†å‰©ä½™chunk\n",
    "        sub_chunk_size = max(1, len(chunk_buffer) // num_gpus)\n",
    "        chunks = [\n",
    "            chunk_buffer[j:j+sub_chunk_size] \n",
    "            for j in range(0, len(chunk_buffer), sub_chunk_size)\n",
    "        ]\n",
    "        \n",
    "        processed_paragraphs = process_with_multi_gpu(chunks, processing_threshold, min(num_gpus, len(chunks)))\n",
    "        processed_buffer.extend(processed_paragraphs)\n",
    "    \n",
    "    # ä¿å­˜å‰©ä½™æ•°æ®\n",
    "    if processed_buffer:\n",
    "        save_paragraphs_incrementally(\n",
    "            processed_buffer, \n",
    "            local_name_ultra, \n",
    "            preprocessed_save_dir_ultra,\n",
    "            batch_size=save_batch_size,\n",
    "            start_part=total_saved_parts\n",
    "        )\n",
    "    \n",
    "    # æœ€ç»ˆç»Ÿè®¡\n",
    "    total_time = time.time() - start_time\n",
    "    avg_speed = total_processed / total_time\n",
    "    \n",
    "    print(f\"\\\\nğŸ‰ğŸ‰ğŸ‰ğŸ‰ å¤šGPUè¶…å¿«å¤„ç†å®Œæˆ!\")\n",
    "    print(f\"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:\")\n",
    "    print(f\"  æ€»å¤„ç†æ—¶é—´: {total_time:.2f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)\")\n",
    "    print(f\"  æ€»å¤„ç†æ ·æœ¬: {total_processed:,} ä¸ª\")\n",
    "    print(f\"  å¹³å‡å¤„ç†é€Ÿåº¦: {avg_speed:.0f} æ ·æœ¬/ç§’\")\n",
    "    print(f\"  ä¿å­˜çš„parquetæ–‡ä»¶æ•°é‡: {total_saved_parts}\")\n",
    "    print(f\"  ğŸš€ğŸš€ğŸš€ğŸš€ 4GPUåŠ é€Ÿæ•ˆæœæ˜¾è‘—!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ å¤šGPUå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf55b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ å¼‚æ­¥å¤šGPUæé€Ÿç‰ˆæœ¬ï¼ˆå®éªŒæ€§ï¼‰\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "async def async_multi_gpu_processing():\n",
    "    \"\"\"å¼‚æ­¥å¤šGPUå¤„ç†ï¼Œæœ€å¤§åŒ–GPUåˆ©ç”¨ç‡\"\"\"\n",
    "    local_name_extreme = \"c4_15m_extreme\"\n",
    "    \n",
    "    # æé™å‚æ•°è®¾ç½®\n",
    "    streaming_chunk_size = 50000    # æ›´å¤§çš„chunk\n",
    "    processing_threshold = 512\n",
    "    save_batch_size = 2000000      # æ›´å¤§çš„ä¿å­˜æ‰¹æ¬¡\n",
    "    num_gpus = 4\n",
    "    \n",
    "    print(f\"ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ å¯åŠ¨å¼‚æ­¥4GPUæé€Ÿå¤„ç†æ¨¡å¼!\")\n",
    "    print(f\"æµå¼å—å¤§å°: {streaming_chunk_size:,}\")\n",
    "    print(f\"ä¿å­˜æ‰¹æ¬¡å¤§å°: {save_batch_size:,}\")\n",
    "    \n",
    "    # è®¾ç½®ä¿å­˜ç›®å½•\n",
    "    ori_save_dir_extreme = os.path.join(ori_datasets_dir, local_name_extreme)\n",
    "    preprocessed_save_dir_extreme = os.path.join(preprocessed_datasets_dir, local_name_extreme)\n",
    "    os.makedirs(ori_save_dir_extreme, exist_ok=True)\n",
    "    os.makedirs(preprocessed_save_dir_extreme, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # åŠ è½½æ•°æ®é›†\n",
    "        dataset_stream = datasets.load_dataset(\"teven/c4_15M\", cache_dir=ori_save_dir_extreme, streaming=True)\n",
    "        train_stream = dataset_stream[\"train\"]\n",
    "        \n",
    "        # åˆ›å»ºçº¿ç¨‹æ± æ‰§è¡Œå™¨\n",
    "        executor = ThreadPoolExecutor(max_workers=num_gpus * 2)\n",
    "        \n",
    "        # åˆå§‹åŒ–\n",
    "        total_processed = 0\n",
    "        total_saved_parts = 0\n",
    "        chunk_buffer = []\n",
    "        processing_tasks = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        async def process_chunk_async(chunk_data, chunk_id):\n",
    "            \"\"\"å¼‚æ­¥å¤„ç†å•ä¸ªchunk\"\"\"\n",
    "            loop = asyncio.get_event_loop()\n",
    "            \n",
    "            # å°†chunkåˆ†å‰²ç»™å¤šä¸ªGPU\n",
    "            sub_chunks = [chunk_data[i::num_gpus] for i in range(num_gpus)]\n",
    "            sub_chunks = [chunk for chunk in sub_chunks if chunk]  # è¿‡æ»¤ç©ºchunk\n",
    "            \n",
    "            # å¹¶è¡Œå¤„ç†\n",
    "            tasks = []\n",
    "            for i, sub_chunk in enumerate(sub_chunks):\n",
    "                task = loop.run_in_executor(\n",
    "                    executor, \n",
    "                    process_text_chunk_multi_gpu, \n",
    "                    sub_chunk, \n",
    "                    processing_threshold, \n",
    "                    i % num_gpus\n",
    "                )\n",
    "                tasks.append(task)\n",
    "            \n",
    "            # ç­‰å¾…æ‰€æœ‰å­ä»»åŠ¡å®Œæˆ\n",
    "            results = await asyncio.gather(*tasks)\n",
    "            \n",
    "            # åˆå¹¶ç»“æœ\n",
    "            all_paragraphs = []\n",
    "            for result in results:\n",
    "                all_paragraphs.extend(result)\n",
    "            \n",
    "            return chunk_id, all_paragraphs\n",
    "        \n",
    "        print(\"ğŸš€ å¼€å§‹å¼‚æ­¥æµå¼å¤„ç†...\")\n",
    "        \n",
    "        chunk_id = 0\n",
    "        processed_buffer = []\n",
    "        \n",
    "        # æµå¼å¤„ç†\n",
    "        for i, item in enumerate(tqdm(train_stream, desc=\"ğŸ”¥å¼‚æ­¥å¤šGPUæé€Ÿå¤„ç†\")):\n",
    "            chunk_buffer.append(item[\"text\"])\n",
    "            \n",
    "            # å½“è¾¾åˆ°chunkå¤§å°æ—¶ï¼Œå¯åŠ¨å¼‚æ­¥å¤„ç†\n",
    "            if len(chunk_buffer) >= streaming_chunk_size:\n",
    "                # å¯åŠ¨å¼‚æ­¥å¤„ç†ä»»åŠ¡\n",
    "                task = asyncio.create_task(\n",
    "                    process_chunk_async(chunk_buffer.copy(), chunk_id)\n",
    "                )\n",
    "                processing_tasks.append(task)\n",
    "                \n",
    "                chunk_id += 1\n",
    "                total_processed += len(chunk_buffer)\n",
    "                chunk_buffer.clear()\n",
    "                \n",
    "                # æ£€æŸ¥å®Œæˆçš„ä»»åŠ¡\n",
    "                done_tasks = [task for task in processing_tasks if task.done()]\n",
    "                for task in done_tasks:\n",
    "                    try:\n",
    "                        task_id, paragraphs = await task\n",
    "                        processed_buffer.extend(paragraphs)\n",
    "                        processing_tasks.remove(task)\n",
    "                        \n",
    "                        # ä¿å­˜æ•°æ®\n",
    "                        if len(processed_buffer) >= save_batch_size:\n",
    "                            save_paragraphs_incrementally(\n",
    "                                processed_buffer, \n",
    "                                local_name_extreme, \n",
    "                                preprocessed_save_dir_extreme,\n",
    "                                batch_size=save_batch_size,\n",
    "                                start_part=total_saved_parts\n",
    "                            )\n",
    "                            total_saved_parts += (len(processed_buffer) + save_batch_size - 1) // save_batch_size\n",
    "                            processed_buffer.clear()\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"å¼‚æ­¥ä»»åŠ¡å¤„ç†é”™è¯¯: {e}\")\n",
    "                        processing_tasks.remove(task)\n",
    "                \n",
    "                # é™åˆ¶å¹¶å‘ä»»åŠ¡æ•°é‡\n",
    "                if len(processing_tasks) > num_gpus * 2:\n",
    "                    # ç­‰å¾…ä¸€äº›ä»»åŠ¡å®Œæˆ\n",
    "                    done, pending = await asyncio.wait(\n",
    "                        processing_tasks, \n",
    "                        return_when=asyncio.FIRST_COMPLETED\n",
    "                    )\n",
    "                    \n",
    "                    for task in done:\n",
    "                        try:\n",
    "                            task_id, paragraphs = task.result()\n",
    "                            processed_buffer.extend(paragraphs)\n",
    "                            processing_tasks.remove(task)\n",
    "                        except Exception as e:\n",
    "                            print(f\"å¼‚æ­¥ä»»åŠ¡å¤„ç†é”™è¯¯: {e}\")\n",
    "                            processing_tasks.remove(task)\n",
    "                \n",
    "                # æ€§èƒ½æŠ¥å‘Š\n",
    "                if total_processed % (streaming_chunk_size * 5) == 0:\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    speed = total_processed / elapsed_time\n",
    "                    print(f\"ğŸ“Š å·²å¤„ç†: {total_processed:,}, é€Ÿåº¦: {speed:.0f} æ ·æœ¬/ç§’, å¾…å¤„ç†ä»»åŠ¡: {len(processing_tasks)}\")\n",
    "        \n",
    "        # å¤„ç†å‰©ä½™chunk\n",
    "        if chunk_buffer:\n",
    "            task = asyncio.create_task(\n",
    "                process_chunk_async(chunk_buffer, chunk_id)\n",
    "            )\n",
    "            processing_tasks.append(task)\n",
    "        \n",
    "        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ\n",
    "        print(\"ğŸ”„ ç­‰å¾…æ‰€æœ‰GPUä»»åŠ¡å®Œæˆ...\")\n",
    "        for task in processing_tasks:\n",
    "            try:\n",
    "                task_id, paragraphs = await task\n",
    "                processed_buffer.extend(paragraphs)\n",
    "            except Exception as e:\n",
    "                print(f\"æœ€ç»ˆä»»åŠ¡å¤„ç†é”™è¯¯: {e}\")\n",
    "        \n",
    "        # ä¿å­˜å‰©ä½™æ•°æ®\n",
    "        if processed_buffer:\n",
    "            save_paragraphs_incrementally(\n",
    "                processed_buffer, \n",
    "                local_name_extreme, \n",
    "                preprocessed_save_dir_extreme,\n",
    "                batch_size=save_batch_size,\n",
    "                start_part=total_saved_parts\n",
    "            )\n",
    "        \n",
    "        # æœ€ç»ˆç»Ÿè®¡\n",
    "        total_time = time.time() - start_time\n",
    "        avg_speed = total_processed / total_time\n",
    "        \n",
    "        print(f\"\\\\nğŸ‰ğŸ‰ğŸ‰ğŸ‰ å¼‚æ­¥å¤šGPUæé€Ÿå¤„ç†å®Œæˆ!\")\n",
    "        print(f\"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:\")\n",
    "        print(f\"  æ€»å¤„ç†æ—¶é—´: {total_time:.2f} ç§’ ({total_time/60:.1f} åˆ†é’Ÿ)\")\n",
    "        print(f\"  æ€»å¤„ç†æ ·æœ¬: {total_processed:,} ä¸ª\")\n",
    "        print(f\"  å¹³å‡å¤„ç†é€Ÿåº¦: {avg_speed:.0f} æ ·æœ¬/ç§’\")\n",
    "        print(f\"  ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ å¼‚æ­¥4GPUæé€Ÿæ•ˆæœ!\")\n",
    "        \n",
    "        executor.shutdown(wait=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¼‚æ­¥å¤šGPUå¤„ç†é”™è¯¯: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# è¿è¡Œå¼‚æ­¥å¤„ç†ï¼ˆå¯é€‰æ‹©è¿è¡Œï¼‰\n",
    "# await async_multi_gpu_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac735af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” GPUåˆ©ç”¨ç‡ç›‘æ§å·¥å…·\n",
    "def monitor_gpu_usage():\n",
    "    \"\"\"ç›‘æ§GPUä½¿ç”¨æƒ…å†µ\"\"\"\n",
    "    try:\n",
    "        import pynvml\n",
    "        pynvml.nvmlInit()\n",
    "        \n",
    "        print(\"ğŸ” GPUä½¿ç”¨æƒ…å†µç›‘æ§:\")\n",
    "        for i in range(device_count):\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            \n",
    "            # è·å–GPUä¿¡æ¯\n",
    "            name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')\n",
    "            \n",
    "            # è·å–å†…å­˜ä¿¡æ¯\n",
    "            meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            memory_used = meminfo.used / 1024**3\n",
    "            memory_total = meminfo.total / 1024**3\n",
    "            memory_percent = (meminfo.used / meminfo.total) * 100\n",
    "            \n",
    "            # è·å–GPUåˆ©ç”¨ç‡\n",
    "            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "            gpu_util = utilization.gpu\n",
    "            \n",
    "            # è·å–æ¸©åº¦\n",
    "            temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "            \n",
    "            print(f\"  GPU {i} ({name}):\")\n",
    "            print(f\"    ğŸ’¾ å†…å­˜: {memory_used:.1f}GB / {memory_total:.1f}GB ({memory_percent:.1f}%)\")\n",
    "            print(f\"    ğŸ”¥ åˆ©ç”¨ç‡: {gpu_util}%\")\n",
    "            print(f\"    ğŸŒ¡ï¸  æ¸©åº¦: {temp}Â°C\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"âš ï¸  pynvmlæœªå®‰è£…ï¼Œæ— æ³•ç›‘æ§GPUã€‚å¯è¿è¡Œ: pip install pynvml\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  GPUç›‘æ§å‡ºé”™: {e}\")\n",
    "\n",
    "# è¿è¡ŒGPUç›‘æ§\n",
    "monitor_gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed169a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ æ€§èƒ½æµ‹è¯•å¯¹æ¯”ï¼ˆå°è§„æ¨¡æµ‹è¯•ï¼‰\n",
    "def performance_benchmark(test_size: int = 10000):\n",
    "    \"\"\"æ€§èƒ½åŸºå‡†æµ‹è¯•\"\"\"\n",
    "    print(f\"ğŸ å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯• (æµ‹è¯•è§„æ¨¡: {test_size:,} æ ·æœ¬)\")\n",
    "    \n",
    "    # ç”Ÿæˆæµ‹è¯•æ•°æ®\n",
    "    test_data = []\n",
    "    for i in range(test_size):\n",
    "        # ç”Ÿæˆä¸åŒé•¿åº¦çš„æµ‹è¯•æ–‡æœ¬\n",
    "        if i % 3 == 0:\n",
    "            text = \"çŸ­æ–‡æœ¬æµ‹è¯• \" * 10  # çŸ­æ–‡æœ¬\n",
    "        elif i % 3 == 1:\n",
    "            text = \"ä¸­ç­‰é•¿åº¦æ–‡æœ¬æµ‹è¯• \" * 50  # ä¸­ç­‰æ–‡æœ¬\n",
    "        else:\n",
    "            text = \"é•¿æ–‡æœ¬æµ‹è¯•éœ€è¦åˆ†å‰² \" * 100 + \"\\\\n\" + \"ç¬¬äºŒæ®µ \" * 100  # é•¿æ–‡æœ¬\n",
    "        test_data.append(text)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. CPUå¤„ç†æµ‹è¯•\n",
    "    print(\"ğŸ”„ æµ‹è¯•CPUå¤„ç†...\")\n",
    "    start_time = time.time()\n",
    "    cpu_result = process_text_chunk_cpu(test_data, 512)\n",
    "    cpu_time = time.time() - start_time\n",
    "    results[\"CPU\"] = {\"time\": cpu_time, \"speed\": len(test_data)/cpu_time, \"output\": len(cpu_result)}\n",
    "    \n",
    "    # 2. å•GPUå¤„ç†æµ‹è¯•\n",
    "    if use_gpu:\n",
    "        print(\"ğŸ”„ æµ‹è¯•å•GPUå¤„ç†...\")\n",
    "        start_time = time.time()\n",
    "        single_gpu_result = process_text_chunk_multi_gpu(test_data, 512, 0)\n",
    "        single_gpu_time = time.time() - start_time\n",
    "        results[\"å•GPU\"] = {\"time\": single_gpu_time, \"speed\": len(test_data)/single_gpu_time, \"output\": len(single_gpu_result)}\n",
    "    \n",
    "    # 3. å¤šGPUå¤„ç†æµ‹è¯•\n",
    "    if use_gpu and device_count >= 4:\n",
    "        print(\"ğŸ”„ æµ‹è¯•4GPUå¤„ç†...\")\n",
    "        # å°†æ•°æ®åˆ†æˆ4ä¸ªchunk\n",
    "        chunk_size = len(test_data) // 4\n",
    "        chunks = [test_data[i:i+chunk_size] for i in range(0, len(test_data), chunk_size)]\n",
    "        if len(chunks) > 4:\n",
    "            chunks[3].extend(chunks[4])\n",
    "            chunks = chunks[:4]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        multi_gpu_result = process_with_multi_gpu(chunks, 512, 4)\n",
    "        multi_gpu_time = time.time() - start_time\n",
    "        results[\"4GPU\"] = {\"time\": multi_gpu_time, \"speed\": len(test_data)/multi_gpu_time, \"output\": len(multi_gpu_result)}\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print(f\"\\\\nğŸ“Š æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ:\")\n",
    "    print(f\"{'æ–¹æ³•':<10} {'æ—¶é—´(ç§’)':<10} {'é€Ÿåº¦(æ ·æœ¬/ç§’)':<15} {'è¾“å‡ºæ®µè½æ•°':<10} {'åŠ é€Ÿæ¯”':<8}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    baseline_speed = results[\"CPU\"][\"speed\"]\n",
    "    for method, result in results.items():\n",
    "        speedup = result[\"speed\"] / baseline_speed\n",
    "        print(f\"{method:<10} {result['time']:<10.2f} {result['speed']:<15.0f} {result['output']:<10,} {speedup:<8.1f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# è¿è¡Œæ€§èƒ½æµ‹è¯•\n",
    "benchmark_results = performance_benchmark(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ae6fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUåŠ é€Ÿç‰ˆæœ¬çš„c4_15mæ•°æ®é›†å¤„ç†\n",
    "local_name_gpu: str = \"c4_15m_gpu\"\n",
    "\n",
    "# è®¾ç½®å‚æ•°\n",
    "streaming_chunk_size = 1_000_000  # æ¯æ¬¡æµå¼åŠ è½½çš„æ ·æœ¬æ•°\n",
    "processing_threshold = 512   # æ–‡æœ¬é•¿åº¦é˜ˆå€¼\n",
    "save_batch_size = 2_000_000     # ä¿å­˜æ‰¹æ¬¡å¤§å°\n",
    "\n",
    "print(f\"å¼€å§‹GPUåŠ é€Ÿå¤„ç† {local_name_gpu} æ•°æ®é›†\")\n",
    "print(f\"æµå¼å—å¤§å°: {streaming_chunk_size:,}\")\n",
    "print(f\"æ–‡æœ¬é•¿åº¦é˜ˆå€¼: {processing_threshold}\")\n",
    "print(f\"ä¿å­˜æ‰¹æ¬¡å¤§å°: {save_batch_size:,}\")\n",
    "\n",
    "# è®¾ç½®ä¿å­˜ç›®å½•\n",
    "ori_save_dir_gpu = os.path.join(ori_datasets_dir, local_name_gpu)\n",
    "preprocessed_save_dir_gpu = os.path.join(preprocessed_datasets_dir, local_name_gpu)\n",
    "os.makedirs(ori_save_dir_gpu, exist_ok=True)\n",
    "os.makedirs(preprocessed_save_dir_gpu, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28511536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æµå¼åŠ è½½å’Œå¤„ç†æ•°æ®é›†\n",
    "try:\n",
    "    # ä½¿ç”¨streamingæ¨¡å¼åŠ è½½æ•°æ®é›†\n",
    "    print(\"å¼€å§‹æµå¼åŠ è½½c4_15Mæ•°æ®é›†...\")\n",
    "    dataset_stream = datasets.load_dataset(\"teven/c4_15M\", cache_dir=ori_save_dir_gpu, streaming=True)\n",
    "    train_stream = dataset_stream[\"train\"]\n",
    "    \n",
    "    # åˆå§‹åŒ–è®¡æ•°å™¨\n",
    "    total_processed = 0\n",
    "    total_saved_parts = 0\n",
    "    current_chunk = []\n",
    "    \n",
    "    print(\"å¼€å§‹æµå¼å¤„ç†...\")\n",
    "    \n",
    "    # æµå¼å¤„ç†æ•°æ®\n",
    "    for i, item in enumerate(tqdm(train_stream, desc=\"æµå¼å¤„ç†\")):\n",
    "        current_chunk.append(item[\"text\"])\n",
    "        \n",
    "        # å½“è¾¾åˆ°chunkå¤§å°æ—¶ï¼Œå¤„ç†è¿™ä¸ªchunk\n",
    "        if len(current_chunk) >= streaming_chunk_size:\n",
    "            # å¤„ç†å½“å‰chunk\n",
    "            processed_paragraphs = process_text_chunk_gpu(current_chunk, processing_threshold)\n",
    "            \n",
    "            # å¦‚æœå¤„ç†åçš„æ®µè½è¶³å¤Ÿå¤šï¼Œå°±ä¿å­˜\n",
    "            if len(processed_paragraphs) >= save_batch_size:\n",
    "                save_paragraphs_incrementally(\n",
    "                    processed_paragraphs, \n",
    "                    local_name_gpu, \n",
    "                    preprocessed_save_dir_gpu,\n",
    "                    batch_size=save_batch_size,\n",
    "                    start_part=total_saved_parts\n",
    "                )\n",
    "                total_saved_parts += (len(processed_paragraphs) + save_batch_size - 1) // save_batch_size\n",
    "                processed_paragraphs.clear()\n",
    "            \n",
    "            total_processed += len(current_chunk)\n",
    "            current_chunk.clear()\n",
    "            \n",
    "            # å†…å­˜æ¸…ç†\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            \n",
    "            # æ˜¾ç¤ºè¿›åº¦\n",
    "            if total_processed % (streaming_chunk_size * 10) == 0:\n",
    "                print(f\"å·²å¤„ç† {total_processed:,} ä¸ªåŸå§‹æ ·æœ¬ï¼Œå·²ä¿å­˜ {total_saved_parts} ä¸ªparquetæ–‡ä»¶\")\n",
    "    \n",
    "    # å¤„ç†å‰©ä½™çš„æ•°æ®\n",
    "    if current_chunk:\n",
    "        processed_paragraphs = process_text_chunk_gpu(current_chunk, processing_threshold)\n",
    "        if processed_paragraphs:\n",
    "            save_paragraphs_incrementally(\n",
    "                processed_paragraphs, \n",
    "                local_name_gpu, \n",
    "                preprocessed_save_dir_gpu,\n",
    "                batch_size=save_batch_size,\n",
    "                start_part=total_saved_parts\n",
    "            )\n",
    "    \n",
    "    print(f\"\\\\næµå¼å¤„ç†å®Œæˆï¼\")\n",
    "    print(f\"æ€»å…±å¤„ç†äº† {total_processed:,} ä¸ªåŸå§‹æ ·æœ¬\")\n",
    "    print(f\"ä¿å­˜çš„parquetæ–‡ä»¶æ•°é‡: {total_saved_parts}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dfa13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set local name\n",
    "local_name: str = \"c4_15m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download original datasets\n",
    "ori_save_dir: str = os.path.join(ori_datasets_dir, local_name)\n",
    "os.makedirs(ori_save_dir, exist_ok=True)\n",
    "\n",
    "c4_15m: Any = download_ori_dataset(\"teven/c4_15M\", cache_dir=ori_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic information\n",
    "print(type(c4_15m), c4_15m.keys())\n",
    "print(type(c4_15m[\"train\"]), type(c4_15m[\"train\"][0]))\n",
    "print(c4_15m[\"train\"][0].keys(), type(c4_15m[\"train\"][0][\"text\"]))\n",
    "print(c4_15m[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd205013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only text for training\n",
    "c4_15m = c4_15m[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54fbdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization for large dataset\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current memory usage.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    print(f\"Current memory usage: {memory_mb:.1f} MB\")\n",
    "\n",
    "print_memory_usage()\n",
    "gc.collect()  # Force garbage collection before processing\n",
    "print(f\"Processing c4_15m dataset with {len(c4_15m):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show length distribution\n",
    "show_sample_length_distribution(c4_15m, split_percent=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a1cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process: split samples by paragraphs (using smaller batch size for c4_15m)\n",
    "print(f\"Starting preprocessing of {len(c4_15m):,} samples...\")\n",
    "c4_15m = split_sample_by_paragraphs(c4_15m, threshold_length=512, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show length distribution again\n",
    "show_sample_length_distribution(c4_15m, split_percent=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b93d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some samples after pre-processing\n",
    "print(\"\\n\\n\".join(c4_15m[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pre-processed data as Parquet files\n",
    "batch_size: int = 10_000\n",
    "save_dir: str = os.path.join(preprocessed_datasets_dir, local_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_as_parquet(local_name, c4_15m, batch_size=batch_size, save_dir=save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

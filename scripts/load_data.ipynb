{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, List\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be75440",
   "metadata": {},
   "source": [
    "# Download & Pre-process Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a8f51",
   "metadata": {},
   "source": [
    "## open-text-books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73137483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set local name\n",
    "local_name: str = \"open-text-books\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f2d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download original dataset\n",
    "ori_save_dir: str = os.path.join(ori_datasets_dir, local_name)\n",
    "os.makedirs(ori_save_dir, exist_ok=True)\n",
    "\n",
    "open_text_books: Any = download_ori_dataset(\"izumi-lab/open-text-books\", cache_dir=ori_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce6a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic information\n",
    "print(type(open_text_books), open_text_books.keys())\n",
    "print(type(open_text_books[\"train\"]), type(open_text_books[\"train\"][0]))\n",
    "print(open_text_books[\"train\"][0].keys(), type(open_text_books[\"train\"][0][\"text\"]))\n",
    "print(open_text_books[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only text for training\n",
    "open_text_books = open_text_books[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13997893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show length distribution\n",
    "show_sample_length_distribution(open_text_books, split_percent=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e5833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process: split samples by paragraphs\n",
    "open_text_books = split_sample_by_paragraphs(open_text_books, threshold_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show length distribution again\n",
    "show_sample_length_distribution(open_text_books, split_percent=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9e3813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some samples after pre-processing\n",
    "print(\"\\n\\n\".join(open_text_books[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6f0f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pre-processed data as Parquet files\n",
    "batch_size: int = 10_000_000\n",
    "save_dir: str = os.path.join(preprocessed_datasets_dir, local_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_as_parquet(local_name, open_text_books, batch_size=batch_size, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac3a168",
   "metadata": {},
   "source": [
    "## c4-subset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef02c3",
   "metadata": {},
   "source": [
    "## GPU加速预处理工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d75db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU加速预处理环境检测\n",
    "import torch\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# 检测是否有GPU可用\n",
    "gpu_available = torch.cuda.is_available()\n",
    "device_count = torch.cuda.device_count() if gpu_available else 0\n",
    "cpu_count = mp.cpu_count()\n",
    "\n",
    "print(f\"GPU可用: {gpu_available}\")\n",
    "if gpu_available:\n",
    "    print(f\"GPU数量: {device_count}\")\n",
    "    for i in range(device_count):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "        print(f\"GPU {i} 内存: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n",
    "print(f\"CPU核心数: {cpu_count}\")\n",
    "\n",
    "# 设置处理策略\n",
    "use_gpu = gpu_available and device_count > 0\n",
    "use_multiprocess = cpu_count > 1\n",
    "print(f\"使用GPU加速: {use_gpu}\")\n",
    "print(f\"使用多进程: {use_multiprocess}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f965795e",
   "metadata": {},
   "source": [
    "### 可选：安装GPU加速库\n",
    "\n",
    "如果要使用GPU加速，请运行以下命令安装所需的库：\n",
    "\n",
    "```bash\n",
    "# 安装RAPIDS库（包含cuDF）\n",
    "conda install -c rapidsai -c conda-forge -c nvidia cudf python=3.9 cudatoolkit=11.8\n",
    "\n",
    "# 或者使用pip安装（需要已安装CUDA）\n",
    "pip install cudf-cu11 --extra-index-url=https://pypi.nvidia.com\n",
    "pip install cupy-cuda11x\n",
    "```\n",
    "\n",
    "如果没有GPU或安装失败，代码会自动回退到CPU处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cf261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_streaming(dataset_name: str, cache_dir: str, streaming_chunk_size: int = 1000):\n",
    "    \"\"\"流式加载数据集，避免内存溢出\"\"\"\n",
    "    print(f\"开始流式加载数据集: {dataset_name}\")\n",
    "    \n",
    "    # 使用streaming模式加载数据集\n",
    "    dataset = datasets.load_dataset(dataset_name, cache_dir=cache_dir, streaming=True)\n",
    "    \n",
    "    # 将流式数据集转换为迭代器\n",
    "    train_iter = iter(dataset[\"train\"])\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            for _ in range(streaming_chunk_size):\n",
    "                try:\n",
    "                    item = next(train_iter)\n",
    "                    current_chunk.append(item[\"text\"])\n",
    "                except StopIteration:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(current_chunk)\n",
    "                    raise StopIteration\n",
    "            \n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            print(f\"已加载 {len(chunks)} 个数据块，每块 {streaming_chunk_size} 个样本\")\n",
    "            \n",
    "    except StopIteration:\n",
    "        print(f\"数据集加载完成，总共 {len(chunks)} 个数据块\")\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ed013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_chunk_gpu(texts: List[str], threshold_length: int = 512) -> List[str]:\n",
    "    \"\"\"使用GPU加速处理文本块\"\"\"\n",
    "    if not use_gpu:\n",
    "        return process_text_chunk_cpu(texts, threshold_length)\n",
    "    \n",
    "    try:\n",
    "        # 尝试使用cuDF进行GPU加速\n",
    "        import cudf\n",
    "        import cupy as cp\n",
    "        \n",
    "        # 转换为GPU DataFrame\n",
    "        df = cudf.DataFrame({\"text\": texts})\n",
    "        \n",
    "        # 计算文本长度\n",
    "        df[\"length\"] = df[\"text\"].str.len()\n",
    "        \n",
    "        # 分离短文本和长文本\n",
    "        short_texts = df[df[\"length\"] <= threshold_length][\"text\"].to_pandas().tolist()\n",
    "        long_texts = df[df[\"length\"] > threshold_length][\"text\"].to_pandas().tolist()\n",
    "        \n",
    "        # 对长文本进行分割\n",
    "        paragraphs = short_texts.copy()\n",
    "        for text in long_texts:\n",
    "            # 按换行符分割\n",
    "            split_texts = [t.strip() for t in text.split(\"\\\\n\") if t.strip()]\n",
    "            paragraphs.extend(split_texts)\n",
    "        \n",
    "        return paragraphs\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"cuDF不可用，回退到CPU处理\")\n",
    "        return process_text_chunk_cpu(texts, threshold_length)\n",
    "    except Exception as e:\n",
    "        print(f\"GPU处理出错: {e}，回退到CPU处理\")\n",
    "        return process_text_chunk_cpu(texts, threshold_length)\n",
    "\n",
    "def process_text_chunk_cpu(texts: List[str], threshold_length: int = 512) -> List[str]:\n",
    "    \"\"\"CPU处理文本块\"\"\"\n",
    "    paragraphs = []\n",
    "    \n",
    "    for text in texts:\n",
    "        if len(text) <= threshold_length:\n",
    "            paragraphs.append(text)\n",
    "        else:\n",
    "            # 按换行符分割\n",
    "            split_texts = [t.strip() for t in text.split(\"\\\\n\") if t.strip()]\n",
    "            paragraphs.extend(split_texts)\n",
    "    \n",
    "    # 过滤空段落\n",
    "    return [p for p in paragraphs if len(p) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f5937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chunks_parallel(chunks: List[List[str]], threshold_length: int = 512, max_workers: int = None) -> List[str]:\n",
    "    \"\"\"并行处理数据块\"\"\"\n",
    "    if max_workers is None:\n",
    "        max_workers = min(cpu_count, len(chunks))\n",
    "    \n",
    "    print(f\"使用 {max_workers} 个进程并行处理 {len(chunks)} 个数据块\")\n",
    "    \n",
    "    all_paragraphs = []\n",
    "    \n",
    "    if use_multiprocess and len(chunks) > 1:\n",
    "        # 多进程处理\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # 提交所有任务\n",
    "            future_to_chunk = {\n",
    "                executor.submit(process_text_chunk_gpu, chunk, threshold_length): i \n",
    "                for i, chunk in enumerate(chunks)\n",
    "            }\n",
    "            \n",
    "            # 收集结果\n",
    "            with tqdm(total=len(chunks), desc=\"处理数据块\") as pbar:\n",
    "                for future in as_completed(future_to_chunk):\n",
    "                    chunk_idx = future_to_chunk[future]\n",
    "                    try:\n",
    "                        result = future.result()\n",
    "                        all_paragraphs.extend(result)\n",
    "                        pbar.update(1)\n",
    "                        pbar.set_postfix({\"已处理段落\": len(all_paragraphs)})\n",
    "                    except Exception as e:\n",
    "                        print(f\"处理数据块 {chunk_idx} 时出错: {e}\")\n",
    "    else:\n",
    "        # 单进程处理\n",
    "        for i, chunk in enumerate(tqdm(chunks, desc=\"处理数据块\")):\n",
    "            try:\n",
    "                result = process_text_chunk_gpu(chunk, threshold_length)\n",
    "                all_paragraphs.extend(result)\n",
    "            except Exception as e:\n",
    "                print(f\"处理数据块 {i} 时出错: {e}\")\n",
    "    \n",
    "    print(f\"处理完成，总共得到 {len(all_paragraphs):,} 个段落\")\n",
    "    return all_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be72285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_paragraphs_incrementally(paragraphs: List[str], local_name: str, save_dir: str, \n",
    "                                  batch_size: int = 50000, start_part: int = 0) -> None:\n",
    "    \"\"\"渐进式保存段落，避免内存积累\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    total_batches = (len(paragraphs) + batch_size - 1) // batch_size\n",
    "    \n",
    "    for i in range(0, len(paragraphs), batch_size):\n",
    "        batch = paragraphs[i:i + batch_size]\n",
    "        part_num = start_part + i // batch_size\n",
    "        \n",
    "        # 创建数据集\n",
    "        batch_dataset = Dataset.from_dict({\"text\": batch})\n",
    "        \n",
    "        # 保存路径\n",
    "        batch_save_path = os.path.join(save_dir, f\"{local_name}_part_{part_num}.parquet\")\n",
    "        \n",
    "        # 保存\n",
    "        batch_dataset.to_parquet(batch_save_path)\n",
    "        \n",
    "        print(f\"已保存第 {i // batch_size + 1}/{total_batches} 批次到 {batch_save_path}，包含 {len(batch):,} 个样本\")\n",
    "        \n",
    "        # 强制垃圾回收\n",
    "        del batch, batch_dataset\n",
    "        import gc\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91c3d70",
   "metadata": {},
   "source": [
    "## c4-subset (GPU加速版本)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1304352b",
   "metadata": {},
   "source": [
    "## c4-subset (多GPU超快版本) 🚀🚀🚀🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0849ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 多GPU超快处理工具\n",
    "import threading\n",
    "import queue\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "def process_text_chunk_multi_gpu(texts: List[str], threshold_length: int = 512, device_id: int = 0) -> List[str]:\n",
    "    \"\"\"在指定GPU上处理文本块\"\"\"\n",
    "    try:\n",
    "        import cudf\n",
    "        import cupy as cp\n",
    "        \n",
    "        # 设置当前GPU设备\n",
    "        cp.cuda.Device(device_id).use()\n",
    "        \n",
    "        # 转换为GPU DataFrame\n",
    "        df = cudf.DataFrame({\"text\": texts})\n",
    "        \n",
    "        # 计算文本长度\n",
    "        df[\"length\"] = df[\"text\"].str.len()\n",
    "        \n",
    "        # 分离短文本和长文本\n",
    "        short_texts = df[df[\"length\"] <= threshold_length][\"text\"].to_pandas().tolist()\n",
    "        long_texts = df[df[\"length\"] > threshold_length][\"text\"].to_pandas().tolist()\n",
    "        \n",
    "        # 对长文本进行分割\n",
    "        paragraphs = short_texts.copy()\n",
    "        for text in long_texts:\n",
    "            # 按换行符分割\n",
    "            split_texts = [t.strip() for t in text.split(\"\\n\") if t.strip()]\n",
    "            paragraphs.extend(split_texts)\n",
    "        \n",
    "        return paragraphs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"GPU {device_id} 处理出错: {e}，回退到CPU处理\")\n",
    "        return process_text_chunk_cpu(texts, threshold_length)\n",
    "\n",
    "def multi_gpu_worker(gpu_id: int, input_queue: queue.Queue, output_queue: queue.Queue, threshold_length: int):\n",
    "    \"\"\"多GPU工作进程\"\"\"\n",
    "    print(f\"GPU {gpu_id} 工作进程启动\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # 获取任务\n",
    "            task = input_queue.get(timeout=5)\n",
    "            if task is None:  # 结束信号\n",
    "                break\n",
    "                \n",
    "            chunk_id, texts = task\n",
    "            \n",
    "            # 处理文本\n",
    "            start_time = time.time()\n",
    "            result = process_text_chunk_multi_gpu(texts, threshold_length, gpu_id)\n",
    "            process_time = time.time() - start_time\n",
    "            \n",
    "            # 返回结果\n",
    "            output_queue.put((chunk_id, result, gpu_id, process_time))\n",
    "            input_queue.task_done()\n",
    "            \n",
    "        except queue.Empty:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"GPU {gpu_id} 工作进程出错: {e}\")\n",
    "            input_queue.task_done()\n",
    "    \n",
    "    print(f\"GPU {gpu_id} 工作进程结束\")\n",
    "\n",
    "def process_with_multi_gpu(chunks: List[List[str]], threshold_length: int = 512, num_gpus: int = 4) -> List[str]:\n",
    "    \"\"\"使用多GPU并行处理数据块\"\"\"\n",
    "    if not use_gpu or device_count < num_gpus:\n",
    "        print(f\"GPU不足，需要{num_gpus}张卡，实际{device_count}张，回退到单GPU/CPU处理\")\n",
    "        return process_chunks_parallel(chunks, threshold_length)\n",
    "    \n",
    "    print(f\"🚀 启动 {num_gpus} 张GPU超快处理模式!\")\n",
    "    \n",
    "    # 创建任务队列和结果队列\n",
    "    input_queue = queue.Queue(maxsize=num_gpus * 2)  # 限制队列大小避免内存溢出\n",
    "    output_queue = queue.Queue()\n",
    "    \n",
    "    # 启动GPU工作线程\n",
    "    workers = []\n",
    "    for gpu_id in range(num_gpus):\n",
    "        worker = threading.Thread(\n",
    "            target=multi_gpu_worker, \n",
    "            args=(gpu_id, input_queue, output_queue, threshold_length)\n",
    "        )\n",
    "        worker.start()\n",
    "        workers.append(worker)\n",
    "    \n",
    "    # 创建任务提交线程\n",
    "    def submit_tasks():\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            input_queue.put((i, chunk))\n",
    "        \n",
    "        # 发送结束信号\n",
    "        for _ in range(num_gpus):\n",
    "            input_queue.put(None)\n",
    "    \n",
    "    submit_thread = threading.Thread(target=submit_tasks)\n",
    "    submit_thread.start()\n",
    "    \n",
    "    # 收集结果\n",
    "    all_paragraphs = []\n",
    "    results = {}\n",
    "    gpu_stats = {i: {\"chunks\": 0, \"time\": 0} for i in range(num_gpus)}\n",
    "    \n",
    "    with tqdm(total=len(chunks), desc=\"多GPU处理中\") as pbar:\n",
    "        for _ in range(len(chunks)):\n",
    "            chunk_id, result, gpu_id, process_time = output_queue.get()\n",
    "            results[chunk_id] = result\n",
    "            gpu_stats[gpu_id][\"chunks\"] += 1\n",
    "            gpu_stats[gpu_id][\"time\"] += process_time\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # 按顺序合并结果\n",
    "    for i in range(len(chunks)):\n",
    "        all_paragraphs.extend(results[i])\n",
    "    \n",
    "    # 等待所有工作线程结束\n",
    "    submit_thread.join()\n",
    "    for worker in workers:\n",
    "        worker.join()\n",
    "    \n",
    "    # 显示GPU统计信息\n",
    "    print(f\"\\n🎯 多GPU处理完成统计:\")\n",
    "    total_time = 0\n",
    "    for gpu_id, stats in gpu_stats.items():\n",
    "        avg_time = stats[\"time\"] / max(stats[\"chunks\"], 1)\n",
    "        total_time += stats[\"time\"]\n",
    "        print(f\"  GPU {gpu_id}: 处理了 {stats['chunks']} 个块, 平均耗时 {avg_time:.2f}s/块\")\n",
    "    \n",
    "    print(f\"  总处理时间: {total_time:.2f}s\")\n",
    "    print(f\"  平均每GPU: {total_time/num_gpus:.2f}s\")\n",
    "    print(f\"  总共得到 {len(all_paragraphs):,} 个段落\")\n",
    "    \n",
    "    return all_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50741bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀🚀🚀🚀 多GPU超快版本处理\n",
    "local_name_ultra: str = \"c4_15m_ultra\"\n",
    "\n",
    "# 优化参数设置（4GPU版本）\n",
    "streaming_chunk_size = 500_000  # 增大chunk以充分利用GPU\n",
    "processing_threshold = 512\n",
    "save_batch_size = 2_000_000    # 增大保存批次\n",
    "num_gpus = 4                 # 使用4张GPU\n",
    "\n",
    "print(f\"🚀🚀🚀🚀 启动4GPU超快处理模式!\")\n",
    "print(f\"流式块大小: {streaming_chunk_size:,}\")\n",
    "print(f\"文本长度阈值: {processing_threshold}\")\n",
    "print(f\"保存批次大小: {save_batch_size:,}\")\n",
    "print(f\"使用GPU数量: {num_gpus}\")\n",
    "\n",
    "# 设置保存目录\n",
    "ori_save_dir_ultra = os.path.join(ori_datasets_dir, local_name_ultra)\n",
    "preprocessed_save_dir_ultra = os.path.join(preprocessed_datasets_dir, local_name_ultra)\n",
    "os.makedirs(ori_save_dir_ultra, exist_ok=True)\n",
    "os.makedirs(preprocessed_save_dir_ultra, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236ab18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀🚀🚀🚀 多GPU流式处理主循环\n",
    "try:\n",
    "    print(\"🚀 开始多GPU超快流式处理...\")\n",
    "    \n",
    "    # 使用streaming模式加载数据集\n",
    "    dataset_stream = datasets.load_dataset(\"teven/c4_15M\", cache_dir=ori_save_dir_ultra, streaming=True)\n",
    "    train_stream = dataset_stream[\"train\"]\n",
    "    \n",
    "    # 初始化计数器和缓存\n",
    "    total_processed = 0\n",
    "    total_saved_parts = 0\n",
    "    chunk_buffer = []\n",
    "    processed_buffer = []\n",
    "    \n",
    "    # 性能统计\n",
    "    start_time = time.time()\n",
    "    last_report_time = start_time\n",
    "    \n",
    "    print(\"🔥 开始流式处理...\")\n",
    "    \n",
    "    # 流式处理数据\n",
    "    for i, item in enumerate(tqdm(train_stream, desc=\"🚀多GPU超快处理\")):\n",
    "        chunk_buffer.append(item[\"text\"])\n",
    "        \n",
    "        # 当缓存达到chunk大小时，进行处理\n",
    "        if len(chunk_buffer) >= streaming_chunk_size:\n",
    "            # 将数据分成4个子chunk，每个GPU处理一个\n",
    "            sub_chunk_size = len(chunk_buffer) // num_gpus\n",
    "            chunks = [\n",
    "                chunk_buffer[j:j+sub_chunk_size] \n",
    "                for j in range(0, len(chunk_buffer), sub_chunk_size)\n",
    "            ]\n",
    "            \n",
    "            # 如果有剩余数据，添加到最后一个chunk\n",
    "            if len(chunks) > num_gpus:\n",
    "                chunks[num_gpus-1].extend(chunks[num_gpus])\n",
    "                chunks = chunks[:num_gpus]\n",
    "            \n",
    "            # 多GPU并行处理\n",
    "            chunk_start_time = time.time()\n",
    "            processed_paragraphs = process_with_multi_gpu(chunks, processing_threshold, num_gpus)\n",
    "            chunk_process_time = time.time() - chunk_start_time\n",
    "            \n",
    "            # 添加到缓存\n",
    "            processed_buffer.extend(processed_paragraphs)\n",
    "            \n",
    "            # 如果处理后的段落足够多，就保存\n",
    "            if len(processed_buffer) >= save_batch_size:\n",
    "                save_paragraphs_incrementally(\n",
    "                    processed_buffer, \n",
    "                    local_name_ultra, \n",
    "                    preprocessed_save_dir_ultra,\n",
    "                    batch_size=save_batch_size,\n",
    "                    start_part=total_saved_parts\n",
    "                )\n",
    "                total_saved_parts += (len(processed_buffer) + save_batch_size - 1) // save_batch_size\n",
    "                processed_buffer.clear()\n",
    "            \n",
    "            total_processed += len(chunk_buffer)\n",
    "            chunk_buffer.clear()\n",
    "            \n",
    "            # 内存清理\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            \n",
    "            # 性能报告\n",
    "            current_time = time.time()\n",
    "            if current_time - last_report_time >= 30:  # 每30秒报告一次\n",
    "                elapsed_time = current_time - start_time\n",
    "                avg_speed = total_processed / elapsed_time\n",
    "                chunk_speed = streaming_chunk_size / chunk_process_time\n",
    "                \n",
    "                print(f\"\\n📊 性能报告:\")\n",
    "                print(f\"  已处理: {total_processed:,} 个原始样本\")\n",
    "                print(f\"  已保存: {total_saved_parts} 个parquet文件\")\n",
    "                print(f\"  平均速度: {avg_speed:.0f} 样本/秒\")\n",
    "                print(f\"  当前块速度: {chunk_speed:.0f} 样本/秒\")\n",
    "                print(f\"  预计4GPU加速比: {chunk_speed/avg_speed:.1f}x\")\n",
    "                \n",
    "                last_report_time = current_time\n",
    "    \n",
    "    # 处理剩余的数据\n",
    "    if chunk_buffer:\n",
    "        # 处理剩余chunk\n",
    "        sub_chunk_size = max(1, len(chunk_buffer) // num_gpus)\n",
    "        chunks = [\n",
    "            chunk_buffer[j:j+sub_chunk_size] \n",
    "            for j in range(0, len(chunk_buffer), sub_chunk_size)\n",
    "        ]\n",
    "        \n",
    "        processed_paragraphs = process_with_multi_gpu(chunks, processing_threshold, min(num_gpus, len(chunks)))\n",
    "        processed_buffer.extend(processed_paragraphs)\n",
    "    \n",
    "    # 保存剩余数据\n",
    "    if processed_buffer:\n",
    "        save_paragraphs_incrementally(\n",
    "            processed_buffer, \n",
    "            local_name_ultra, \n",
    "            preprocessed_save_dir_ultra,\n",
    "            batch_size=save_batch_size,\n",
    "            start_part=total_saved_parts\n",
    "        )\n",
    "    \n",
    "    # 最终统计\n",
    "    total_time = time.time() - start_time\n",
    "    avg_speed = total_processed / total_time\n",
    "    \n",
    "    print(f\"\\\\n🎉🎉🎉🎉 多GPU超快处理完成!\")\n",
    "    print(f\"📊 最终统计:\")\n",
    "    print(f\"  总处理时间: {total_time:.2f} 秒 ({total_time/60:.1f} 分钟)\")\n",
    "    print(f\"  总处理样本: {total_processed:,} 个\")\n",
    "    print(f\"  平均处理速度: {avg_speed:.0f} 样本/秒\")\n",
    "    print(f\"  保存的parquet文件数量: {total_saved_parts}\")\n",
    "    print(f\"  🚀🚀🚀🚀 4GPU加速效果显著!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 多GPU处理过程中出现错误: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf55b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔥🔥🔥🔥 异步多GPU极速版本（实验性）\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "async def async_multi_gpu_processing():\n",
    "    \"\"\"异步多GPU处理，最大化GPU利用率\"\"\"\n",
    "    local_name_extreme = \"c4_15m_extreme\"\n",
    "    \n",
    "    # 极限参数设置\n",
    "    streaming_chunk_size = 50000    # 更大的chunk\n",
    "    processing_threshold = 512\n",
    "    save_batch_size = 2000000      # 更大的保存批次\n",
    "    num_gpus = 4\n",
    "    \n",
    "    print(f\"🔥🔥🔥🔥 启动异步4GPU极速处理模式!\")\n",
    "    print(f\"流式块大小: {streaming_chunk_size:,}\")\n",
    "    print(f\"保存批次大小: {save_batch_size:,}\")\n",
    "    \n",
    "    # 设置保存目录\n",
    "    ori_save_dir_extreme = os.path.join(ori_datasets_dir, local_name_extreme)\n",
    "    preprocessed_save_dir_extreme = os.path.join(preprocessed_datasets_dir, local_name_extreme)\n",
    "    os.makedirs(ori_save_dir_extreme, exist_ok=True)\n",
    "    os.makedirs(preprocessed_save_dir_extreme, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # 加载数据集\n",
    "        dataset_stream = datasets.load_dataset(\"teven/c4_15M\", cache_dir=ori_save_dir_extreme, streaming=True)\n",
    "        train_stream = dataset_stream[\"train\"]\n",
    "        \n",
    "        # 创建线程池执行器\n",
    "        executor = ThreadPoolExecutor(max_workers=num_gpus * 2)\n",
    "        \n",
    "        # 初始化\n",
    "        total_processed = 0\n",
    "        total_saved_parts = 0\n",
    "        chunk_buffer = []\n",
    "        processing_tasks = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        async def process_chunk_async(chunk_data, chunk_id):\n",
    "            \"\"\"异步处理单个chunk\"\"\"\n",
    "            loop = asyncio.get_event_loop()\n",
    "            \n",
    "            # 将chunk分割给多个GPU\n",
    "            sub_chunks = [chunk_data[i::num_gpus] for i in range(num_gpus)]\n",
    "            sub_chunks = [chunk for chunk in sub_chunks if chunk]  # 过滤空chunk\n",
    "            \n",
    "            # 并行处理\n",
    "            tasks = []\n",
    "            for i, sub_chunk in enumerate(sub_chunks):\n",
    "                task = loop.run_in_executor(\n",
    "                    executor, \n",
    "                    process_text_chunk_multi_gpu, \n",
    "                    sub_chunk, \n",
    "                    processing_threshold, \n",
    "                    i % num_gpus\n",
    "                )\n",
    "                tasks.append(task)\n",
    "            \n",
    "            # 等待所有子任务完成\n",
    "            results = await asyncio.gather(*tasks)\n",
    "            \n",
    "            # 合并结果\n",
    "            all_paragraphs = []\n",
    "            for result in results:\n",
    "                all_paragraphs.extend(result)\n",
    "            \n",
    "            return chunk_id, all_paragraphs\n",
    "        \n",
    "        print(\"🚀 开始异步流式处理...\")\n",
    "        \n",
    "        chunk_id = 0\n",
    "        processed_buffer = []\n",
    "        \n",
    "        # 流式处理\n",
    "        for i, item in enumerate(tqdm(train_stream, desc=\"🔥异步多GPU极速处理\")):\n",
    "            chunk_buffer.append(item[\"text\"])\n",
    "            \n",
    "            # 当达到chunk大小时，启动异步处理\n",
    "            if len(chunk_buffer) >= streaming_chunk_size:\n",
    "                # 启动异步处理任务\n",
    "                task = asyncio.create_task(\n",
    "                    process_chunk_async(chunk_buffer.copy(), chunk_id)\n",
    "                )\n",
    "                processing_tasks.append(task)\n",
    "                \n",
    "                chunk_id += 1\n",
    "                total_processed += len(chunk_buffer)\n",
    "                chunk_buffer.clear()\n",
    "                \n",
    "                # 检查完成的任务\n",
    "                done_tasks = [task for task in processing_tasks if task.done()]\n",
    "                for task in done_tasks:\n",
    "                    try:\n",
    "                        task_id, paragraphs = await task\n",
    "                        processed_buffer.extend(paragraphs)\n",
    "                        processing_tasks.remove(task)\n",
    "                        \n",
    "                        # 保存数据\n",
    "                        if len(processed_buffer) >= save_batch_size:\n",
    "                            save_paragraphs_incrementally(\n",
    "                                processed_buffer, \n",
    "                                local_name_extreme, \n",
    "                                preprocessed_save_dir_extreme,\n",
    "                                batch_size=save_batch_size,\n",
    "                                start_part=total_saved_parts\n",
    "                            )\n",
    "                            total_saved_parts += (len(processed_buffer) + save_batch_size - 1) // save_batch_size\n",
    "                            processed_buffer.clear()\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"异步任务处理错误: {e}\")\n",
    "                        processing_tasks.remove(task)\n",
    "                \n",
    "                # 限制并发任务数量\n",
    "                if len(processing_tasks) > num_gpus * 2:\n",
    "                    # 等待一些任务完成\n",
    "                    done, pending = await asyncio.wait(\n",
    "                        processing_tasks, \n",
    "                        return_when=asyncio.FIRST_COMPLETED\n",
    "                    )\n",
    "                    \n",
    "                    for task in done:\n",
    "                        try:\n",
    "                            task_id, paragraphs = task.result()\n",
    "                            processed_buffer.extend(paragraphs)\n",
    "                            processing_tasks.remove(task)\n",
    "                        except Exception as e:\n",
    "                            print(f\"异步任务处理错误: {e}\")\n",
    "                            processing_tasks.remove(task)\n",
    "                \n",
    "                # 性能报告\n",
    "                if total_processed % (streaming_chunk_size * 5) == 0:\n",
    "                    elapsed_time = time.time() - start_time\n",
    "                    speed = total_processed / elapsed_time\n",
    "                    print(f\"📊 已处理: {total_processed:,}, 速度: {speed:.0f} 样本/秒, 待处理任务: {len(processing_tasks)}\")\n",
    "        \n",
    "        # 处理剩余chunk\n",
    "        if chunk_buffer:\n",
    "            task = asyncio.create_task(\n",
    "                process_chunk_async(chunk_buffer, chunk_id)\n",
    "            )\n",
    "            processing_tasks.append(task)\n",
    "        \n",
    "        # 等待所有任务完成\n",
    "        print(\"🔄 等待所有GPU任务完成...\")\n",
    "        for task in processing_tasks:\n",
    "            try:\n",
    "                task_id, paragraphs = await task\n",
    "                processed_buffer.extend(paragraphs)\n",
    "            except Exception as e:\n",
    "                print(f\"最终任务处理错误: {e}\")\n",
    "        \n",
    "        # 保存剩余数据\n",
    "        if processed_buffer:\n",
    "            save_paragraphs_incrementally(\n",
    "                processed_buffer, \n",
    "                local_name_extreme, \n",
    "                preprocessed_save_dir_extreme,\n",
    "                batch_size=save_batch_size,\n",
    "                start_part=total_saved_parts\n",
    "            )\n",
    "        \n",
    "        # 最终统计\n",
    "        total_time = time.time() - start_time\n",
    "        avg_speed = total_processed / total_time\n",
    "        \n",
    "        print(f\"\\\\n🎉🎉🎉🎉 异步多GPU极速处理完成!\")\n",
    "        print(f\"📊 最终统计:\")\n",
    "        print(f\"  总处理时间: {total_time:.2f} 秒 ({total_time/60:.1f} 分钟)\")\n",
    "        print(f\"  总处理样本: {total_processed:,} 个\")\n",
    "        print(f\"  平均处理速度: {avg_speed:.0f} 样本/秒\")\n",
    "        print(f\"  🔥🔥🔥🔥 异步4GPU极速效果!\")\n",
    "        \n",
    "        executor.shutdown(wait=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 异步多GPU处理错误: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# 运行异步处理（可选择运行）\n",
    "# await async_multi_gpu_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac735af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 GPU利用率监控工具\n",
    "def monitor_gpu_usage():\n",
    "    \"\"\"监控GPU使用情况\"\"\"\n",
    "    try:\n",
    "        import pynvml\n",
    "        pynvml.nvmlInit()\n",
    "        \n",
    "        print(\"🔍 GPU使用情况监控:\")\n",
    "        for i in range(device_count):\n",
    "            handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n",
    "            \n",
    "            # 获取GPU信息\n",
    "            name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')\n",
    "            \n",
    "            # 获取内存信息\n",
    "            meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "            memory_used = meminfo.used / 1024**3\n",
    "            memory_total = meminfo.total / 1024**3\n",
    "            memory_percent = (meminfo.used / meminfo.total) * 100\n",
    "            \n",
    "            # 获取GPU利用率\n",
    "            utilization = pynvml.nvmlDeviceGetUtilizationRates(handle)\n",
    "            gpu_util = utilization.gpu\n",
    "            \n",
    "            # 获取温度\n",
    "            temp = pynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\n",
    "            \n",
    "            print(f\"  GPU {i} ({name}):\")\n",
    "            print(f\"    💾 内存: {memory_used:.1f}GB / {memory_total:.1f}GB ({memory_percent:.1f}%)\")\n",
    "            print(f\"    🔥 利用率: {gpu_util}%\")\n",
    "            print(f\"    🌡️  温度: {temp}°C\")\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"⚠️  pynvml未安装，无法监控GPU。可运行: pip install pynvml\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️  GPU监控出错: {e}\")\n",
    "\n",
    "# 运行GPU监控\n",
    "monitor_gpu_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed169a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏁 性能测试对比（小规模测试）\n",
    "def performance_benchmark(test_size: int = 10000):\n",
    "    \"\"\"性能基准测试\"\"\"\n",
    "    print(f\"🏁 开始性能基准测试 (测试规模: {test_size:,} 样本)\")\n",
    "    \n",
    "    # 生成测试数据\n",
    "    test_data = []\n",
    "    for i in range(test_size):\n",
    "        # 生成不同长度的测试文本\n",
    "        if i % 3 == 0:\n",
    "            text = \"短文本测试 \" * 10  # 短文本\n",
    "        elif i % 3 == 1:\n",
    "            text = \"中等长度文本测试 \" * 50  # 中等文本\n",
    "        else:\n",
    "            text = \"长文本测试需要分割 \" * 100 + \"\\\\n\" + \"第二段 \" * 100  # 长文本\n",
    "        test_data.append(text)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. CPU处理测试\n",
    "    print(\"🔄 测试CPU处理...\")\n",
    "    start_time = time.time()\n",
    "    cpu_result = process_text_chunk_cpu(test_data, 512)\n",
    "    cpu_time = time.time() - start_time\n",
    "    results[\"CPU\"] = {\"time\": cpu_time, \"speed\": len(test_data)/cpu_time, \"output\": len(cpu_result)}\n",
    "    \n",
    "    # 2. 单GPU处理测试\n",
    "    if use_gpu:\n",
    "        print(\"🔄 测试单GPU处理...\")\n",
    "        start_time = time.time()\n",
    "        single_gpu_result = process_text_chunk_multi_gpu(test_data, 512, 0)\n",
    "        single_gpu_time = time.time() - start_time\n",
    "        results[\"单GPU\"] = {\"time\": single_gpu_time, \"speed\": len(test_data)/single_gpu_time, \"output\": len(single_gpu_result)}\n",
    "    \n",
    "    # 3. 多GPU处理测试\n",
    "    if use_gpu and device_count >= 4:\n",
    "        print(\"🔄 测试4GPU处理...\")\n",
    "        # 将数据分成4个chunk\n",
    "        chunk_size = len(test_data) // 4\n",
    "        chunks = [test_data[i:i+chunk_size] for i in range(0, len(test_data), chunk_size)]\n",
    "        if len(chunks) > 4:\n",
    "            chunks[3].extend(chunks[4])\n",
    "            chunks = chunks[:4]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        multi_gpu_result = process_with_multi_gpu(chunks, 512, 4)\n",
    "        multi_gpu_time = time.time() - start_time\n",
    "        results[\"4GPU\"] = {\"time\": multi_gpu_time, \"speed\": len(test_data)/multi_gpu_time, \"output\": len(multi_gpu_result)}\n",
    "    \n",
    "    # 显示结果\n",
    "    print(f\"\\\\n📊 性能基准测试结果:\")\n",
    "    print(f\"{'方法':<10} {'时间(秒)':<10} {'速度(样本/秒)':<15} {'输出段落数':<10} {'加速比':<8}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    baseline_speed = results[\"CPU\"][\"speed\"]\n",
    "    for method, result in results.items():\n",
    "        speedup = result[\"speed\"] / baseline_speed\n",
    "        print(f\"{method:<10} {result['time']:<10.2f} {result['speed']:<15.0f} {result['output']:<10,} {speedup:<8.1f}x\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行性能测试\n",
    "benchmark_results = performance_benchmark(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ae6fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU加速版本的c4_15m数据集处理\n",
    "local_name_gpu: str = \"c4_15m_gpu\"\n",
    "\n",
    "# 设置参数\n",
    "streaming_chunk_size = 1_000_000  # 每次流式加载的样本数\n",
    "processing_threshold = 512   # 文本长度阈值\n",
    "save_batch_size = 2_000_000     # 保存批次大小\n",
    "\n",
    "print(f\"开始GPU加速处理 {local_name_gpu} 数据集\")\n",
    "print(f\"流式块大小: {streaming_chunk_size:,}\")\n",
    "print(f\"文本长度阈值: {processing_threshold}\")\n",
    "print(f\"保存批次大小: {save_batch_size:,}\")\n",
    "\n",
    "# 设置保存目录\n",
    "ori_save_dir_gpu = os.path.join(ori_datasets_dir, local_name_gpu)\n",
    "preprocessed_save_dir_gpu = os.path.join(preprocessed_datasets_dir, local_name_gpu)\n",
    "os.makedirs(ori_save_dir_gpu, exist_ok=True)\n",
    "os.makedirs(preprocessed_save_dir_gpu, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28511536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 流式加载和处理数据集\n",
    "try:\n",
    "    # 使用streaming模式加载数据集\n",
    "    print(\"开始流式加载c4_15M数据集...\")\n",
    "    dataset_stream = datasets.load_dataset(\"teven/c4_15M\", cache_dir=ori_save_dir_gpu, streaming=True)\n",
    "    train_stream = dataset_stream[\"train\"]\n",
    "    \n",
    "    # 初始化计数器\n",
    "    total_processed = 0\n",
    "    total_saved_parts = 0\n",
    "    current_chunk = []\n",
    "    \n",
    "    print(\"开始流式处理...\")\n",
    "    \n",
    "    # 流式处理数据\n",
    "    for i, item in enumerate(tqdm(train_stream, desc=\"流式处理\")):\n",
    "        current_chunk.append(item[\"text\"])\n",
    "        \n",
    "        # 当达到chunk大小时，处理这个chunk\n",
    "        if len(current_chunk) >= streaming_chunk_size:\n",
    "            # 处理当前chunk\n",
    "            processed_paragraphs = process_text_chunk_gpu(current_chunk, processing_threshold)\n",
    "            \n",
    "            # 如果处理后的段落足够多，就保存\n",
    "            if len(processed_paragraphs) >= save_batch_size:\n",
    "                save_paragraphs_incrementally(\n",
    "                    processed_paragraphs, \n",
    "                    local_name_gpu, \n",
    "                    preprocessed_save_dir_gpu,\n",
    "                    batch_size=save_batch_size,\n",
    "                    start_part=total_saved_parts\n",
    "                )\n",
    "                total_saved_parts += (len(processed_paragraphs) + save_batch_size - 1) // save_batch_size\n",
    "                processed_paragraphs.clear()\n",
    "            \n",
    "            total_processed += len(current_chunk)\n",
    "            current_chunk.clear()\n",
    "            \n",
    "            # 内存清理\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            \n",
    "            # 显示进度\n",
    "            if total_processed % (streaming_chunk_size * 10) == 0:\n",
    "                print(f\"已处理 {total_processed:,} 个原始样本，已保存 {total_saved_parts} 个parquet文件\")\n",
    "    \n",
    "    # 处理剩余的数据\n",
    "    if current_chunk:\n",
    "        processed_paragraphs = process_text_chunk_gpu(current_chunk, processing_threshold)\n",
    "        if processed_paragraphs:\n",
    "            save_paragraphs_incrementally(\n",
    "                processed_paragraphs, \n",
    "                local_name_gpu, \n",
    "                preprocessed_save_dir_gpu,\n",
    "                batch_size=save_batch_size,\n",
    "                start_part=total_saved_parts\n",
    "            )\n",
    "    \n",
    "    print(f\"\\\\n流式处理完成！\")\n",
    "    print(f\"总共处理了 {total_processed:,} 个原始样本\")\n",
    "    print(f\"保存的parquet文件数量: {total_saved_parts}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"处理过程中出现错误: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dfa13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set local name\n",
    "local_name: str = \"c4_15m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95f787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download original datasets\n",
    "ori_save_dir: str = os.path.join(ori_datasets_dir, local_name)\n",
    "os.makedirs(ori_save_dir, exist_ok=True)\n",
    "\n",
    "c4_15m: Any = download_ori_dataset(\"teven/c4_15M\", cache_dir=ori_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e30fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show basic information\n",
    "print(type(c4_15m), c4_15m.keys())\n",
    "print(type(c4_15m[\"train\"]), type(c4_15m[\"train\"][0]))\n",
    "print(c4_15m[\"train\"][0].keys(), type(c4_15m[\"train\"][0][\"text\"]))\n",
    "print(c4_15m[\"train\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd205013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only text for training\n",
    "c4_15m = c4_15m[\"train\"][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54fbdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory optimization for large dataset\n",
    "import gc\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "def print_memory_usage():\n",
    "    \"\"\"Print current memory usage.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    print(f\"Current memory usage: {memory_mb:.1f} MB\")\n",
    "\n",
    "print_memory_usage()\n",
    "gc.collect()  # Force garbage collection before processing\n",
    "print(f\"Processing c4_15m dataset with {len(c4_15m):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771c2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show length distribution\n",
    "show_sample_length_distribution(c4_15m, split_percent=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a1cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process: split samples by paragraphs (using smaller batch size for c4_15m)\n",
    "print(f\"Starting preprocessing of {len(c4_15m):,} samples...\")\n",
    "c4_15m = split_sample_by_paragraphs(c4_15m, threshold_length=512, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c6606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show length distribution again\n",
    "show_sample_length_distribution(c4_15m, split_percent=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b93d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some samples after pre-processing\n",
    "print(\"\\n\\n\".join(c4_15m[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7f0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pre-processed data as Parquet files\n",
    "batch_size: int = 10_000\n",
    "save_dir: str = os.path.join(preprocessed_datasets_dir, local_name)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "save_as_parquet(local_name, c4_15m, batch_size=batch_size, save_dir=save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
